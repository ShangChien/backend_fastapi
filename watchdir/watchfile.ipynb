{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time,logging\n",
    "from pathlib import Path\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler\n",
    "from prefect import flow,task\n",
    "from collections import OrderedDict\n",
    "from typing import List,Dict,Any,Tuple,Union,OrderedDict\n",
    "from watchdir.data_api import single_db_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Caches:\n",
    "    def __init__(self, kv:List[Any]=[], max_size:int=10) -> None:\n",
    "        self._max_size = max_size\n",
    "        self._items = OrderedDict()\n",
    "        if kv:\n",
    "            for i in kv:\n",
    "                self.add(i[0], i[1])\n",
    "\n",
    "    @property\n",
    "    def items(self) -> OrderedDict:\n",
    "        return self._items\n",
    "\n",
    "    @property\n",
    "    def max_size(self) -> int:\n",
    "        return self._max_size\n",
    "        \n",
    "    @max_size.setter\n",
    "    def max_size(self,max_size:int) -> None:\n",
    "        if max_size <= 0:\n",
    "            print(\"max_size must be positive\")\n",
    "            return\n",
    "        self._max_size = max_size\n",
    "        if max_size < len(self._items):\n",
    "            self._items=self._items[-max_size:]\n",
    "\n",
    "    def __getitem__(self, index) ->Any:\n",
    "        if isinstance(index, slice):\n",
    "            start, stop, step = index.start, index.stop, index.step\n",
    "            return list(self._items.items())[start:stop:step]\n",
    "        else:\n",
    "            return list(self._items.items())[index]\n",
    "            \n",
    "    def add(self, key:Union[int,str], value:Any) -> None:\n",
    "        if key in self._items:\n",
    "            print(\"Item already exists in the container\")\n",
    "            return\n",
    "        if len(self._items) >= self.max_size:\n",
    "            _, _history = self._items.popitem(last=False)\n",
    "            with open('./.history.log', 'a') as f:\n",
    "                f.write(f'{_history}\\n')\n",
    "        self._items[key] = value\n",
    "\n",
    "    def update_kv(self, key:Union[int,str], value:Any) -> None:\n",
    "        if key not in self._items:\n",
    "            print(\"Item doesn't exist in the container\")\n",
    "            return\n",
    "        self._items[key] = value\n",
    "\n",
    "    def rm(self, key) -> None:\n",
    "        if key not in self._items:\n",
    "            print(\"Item doesn't exist in the container\")\n",
    "            return\n",
    "        del self._items[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class subHandler(FileSystemEventHandler):\n",
    "    def on_created(self, event) -> None:\n",
    "        \n",
    "        if event.is_directory:\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"File created: {event.src_path}\")\n",
    "            path=Path(event.src_path)\n",
    "            if path.suffix == '.log':\n",
    "                single_db_process(path.as_posix())\n",
    "    def on_modified(self, event) -> None:\n",
    "        if event.is_directory:\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"File modied: {event.src_path}\")\n",
    "            path=Path(event.src_path)\n",
    "            if path.suffix == '.log':\n",
    "                single_db_process(path.as_posix())\n",
    "\n",
    "def update_subqueue(sub_queue:Caches) -> None:\n",
    "    folders = [p for p in dir_path.glob('*') if p.is_dir()]\n",
    "    last_updated_dir=sorted(folders, key=lambda p: p.stat().st_mtime, reverse=True)[:3]\n",
    "    for dir in last_updated_dir:\n",
    "        str_dir = dir.as_posix()\n",
    "        if str_dir in sub_queue.items:\n",
    "            continue\n",
    "        else:\n",
    "            ##stop & join older observer if queue is full\n",
    "            if sub_queue.max_size == len(sub_queue.items):\n",
    "                sub_queue[0][1].stop()\n",
    "                sub_queue[0][1].join()\n",
    "            ##add observer(auto delete older observer)\n",
    "            sub_handler = subHandler()\n",
    "            observer = Observer()\n",
    "            observer.schedule(sub_handler, str_dir, recursive=True)\n",
    "            sub_queue.add(str_dir,observer)\n",
    "            observer.start()\n",
    "\n",
    "class mainHandler(FileSystemEventHandler):\n",
    "    def on_created(self, event) -> None:\n",
    "        if event.is_directory:\n",
    "            print(f\"Directory modied: {event.src_path}\")\n",
    "            update_subqueue(sub_queue)\n",
    "    def on_modified(self, event) -> None:\n",
    "        if event.is_directory:\n",
    "            print(f\"Directory modied: {event.src_path}\")\n",
    "            update_subqueue(sub_queue)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.DEBUG, filename='error.log', filemode='w', format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    dir_path =   Path('Z:/Data-month-statistics')\n",
    "    sub_queue  =  Caches([],3)\n",
    "    main_handler = mainHandler()\n",
    "    main_observer = Observer()\n",
    "    main_observer.schedule(main_handler, dir_path.as_posix(), recursive=False)\n",
    "    main_observer.start()\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(5)\n",
    "            for _key,observer in sub_queue.items.items():\n",
    "                try:\n",
    "                    observer.join()\n",
    "                except:\n",
    "                    print('watchdog error join sub_observer:',observer,_key)\n",
    "            main_observer.join()\n",
    "    except KeyboardInterrupt:\n",
    "        for _key,observer in sub_queue.items.items():\n",
    "            try:\n",
    "                observer.stop()\n",
    "                observer.join()\n",
    "            except:\n",
    "                print('watchdog error join & stop sub_observer:',observer,_key)\n",
    "        main_observer.stop()\n",
    "        main_observer.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "class dataUnimol(BaseModel):\n",
    "    models: list[str] | None = []\n",
    "    names : list[str] | None = []\n",
    "    smiles: list[str] | None = []\n",
    "    molBlocks: list[str] | None = []\n",
    "    atoms : list[list[str]] | None = [] \n",
    "    coordinates : list[list[list[float]]] | None = []\n",
    "    results : dict[str, list[int|float]] | None = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataUnimol(models=[], names=[], smiles=[], molBlocks=[], atoms=[], coordinates=[], results={})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=dataUnimol()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a04f5d07b0747026a8fbcdf50b9443318e69b1b8bd6247d88bfadb4789282972"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
