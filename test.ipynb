{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame,Series\n",
    "import math\n",
    "import copy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import signal\n",
    "from numpy import ndarray\n",
    "from pathlib import Path as P\n",
    "from typing import Any\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import voigt_profile\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from pydantic import BaseModel\n",
    "from scipy.optimize import minimize\n",
    "import nest_asyncio\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.sparse import csr_matrix\n",
    "nest_asyncio.apply()\n",
    "\n",
    "class UVdata(BaseModel):\n",
    "    name: str\n",
    "    raw_arr: list[list[int|float]]\n",
    "    peaks_arr: list[int|float] | None = []\n",
    "\n",
    "# 定义要拟合的函数列表\n",
    "class peak_funcs:\n",
    "    \n",
    "    @staticmethod\n",
    "    def exp(x, a, b, c):\n",
    "        return a * np.exp(b * (x-c))\n",
    "    \n",
    "    @staticmethod\n",
    "    def gauss(x, A, mu, sigma):\n",
    "        return A * np.exp(-(x - mu)**2 / (2 * sigma**2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def lorentz(x, A, mu, gamma):\n",
    "        return A / (1 + ((x - mu) / gamma)**2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def voigt(x, A, mu, sigma, gamma):\n",
    "        return A * voigt_profile(x - mu, sigma, gamma)\n",
    "\n",
    "models = [getattr(peak_funcs, i) for i in dir(peak_funcs) if not i.startswith('__')]\n",
    "\n",
    "# 定义损失函数\n",
    "def loss(params, x, y, func):\n",
    "    y_pred = func(x, *params)\n",
    "    diff = y - y_pred\n",
    "    diff = np.nan_to_num(diff)\n",
    "    diff = np.where(np.isnan(diff), 0, diff)\n",
    "    diff = np.where(np.isposinf(diff), 1.0, diff)\n",
    "    diff = np.where(np.isneginf(diff), -1.0, diff)\n",
    "    loss_v = np.sum(np.abs(diff) + np.maximum(0, 10*(diff)))\n",
    "    return loss_v\n",
    "\n",
    "# 读取excel\n",
    "def get_data_from_excel(file:P)-> dict:\n",
    "    filename: str= file.stem\n",
    "    df: DataFrame = pd.read_excel(file, header=None)\n",
    "    df.dropna(axis=0, how='all', inplace=True)\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    mask: DataFrame = df.applymap(lambda x: 'nm' == str(x).strip())    ## 在df中寻找值为字符串'nm'索引\n",
    "    mask_series: DataFrame | Series = mask.stack()                           ## 二维数据打平转化为series(row,col,value)\n",
    "    indices: list[Any] = mask_series[lambda x: x].index # type: ignore\n",
    "    if len(indices) > 1:\n",
    "        raise Exception(f'{filename}: 存在多个值为nm的单元格{indices}')\n",
    "    elif len(indices) == 0:\n",
    "        raise Exception(f'{filename}: 不存在值为nm的单元格')\n",
    "    else:\n",
    "        arr: ndarray = df.loc[indices[0][0]+1:,indices[0][1]:].to_numpy()\n",
    "        return {'name':filename, 'raw_arr': arr}\n",
    "\n",
    "# 数据前处理\n",
    "def pre_process(data:ndarray)-> ndarray:\n",
    "    scaler = MinMaxScaler()\n",
    "    arr_normalized = scaler.fit_transform(data.reshape(-1,1)).reshape(-1)\n",
    "    arr_normalized = signal.savgol_filter(arr_normalized, window_length=10, polyorder=2)\n",
    "    return arr_normalized.tolist(), scaler # type: ignore\n",
    "\n",
    "## 寻找峰值\n",
    "def get_peaks(data:ndarray, threshold=10)-> list[int]:\n",
    "\n",
    "    peaks_normal: ndarray\n",
    "    _property:dict\n",
    "    peaks_normal, _property = signal.find_peaks(data, prominence=0.002, distance=10)\n",
    "    peaks_cwt: ndarray = signal.find_peaks_cwt(data, np.arange(1, 10), min_length=4, min_snr=1)\n",
    "    ## 合并去重,过滤低值\n",
    "    peaks_merged: list[Any] = sorted(list(set(peaks_normal.tolist() + peaks_cwt.tolist())))\n",
    "    peaks=[i for i in peaks_merged if data[i] > 0.05]\n",
    "    if len(peaks) == 0:\n",
    "        return []\n",
    "    ## 筛选主峰\n",
    "    diffs = np.diff(peaks)\n",
    "    separators = np.where(diffs >= threshold)[0] + 1\n",
    "    subarrays= np.split(peaks, separators)\n",
    "    peaks=[]\n",
    "    ## 密集区域稀疏化\n",
    "    for sub in subarrays:\n",
    "        if len(sub) == 1:\n",
    "            sub = sub[0]\n",
    "        else:\n",
    "            value_in_peaks_normal =np.array([i for i in sub if i in peaks_normal])\n",
    "            if len(value_in_peaks_normal) == 0:\n",
    "                sub = int(sub.mean())\n",
    "            else:\n",
    "                index = np.argmin(value_in_peaks_normal - sub.mean())\n",
    "                sub= value_in_peaks_normal[index]\n",
    "        peaks.append(sub)\n",
    "    # print('peaks:',peaks)\n",
    "    return peaks\n",
    "\n",
    "# 迭代寻找峰值主函数\n",
    "def iter_peaks(x_data, y_data, iter_num:int|None = None, results:list[dict] = []) -> list[dict]:\n",
    "    \"\"\"\n",
    "    find the best fitting model for each peak.\n",
    "\n",
    "    Args:\n",
    "        x_data: The x-axis data points.\n",
    "        y_data: The y-axis data points.\n",
    "        iter_num: 最大迭代次数 (optional).\n",
    "        results: 输出的结果 (optional).\n",
    "\n",
    "    Returns:\n",
    "        A list of fitting results, where each result contains:\n",
    "            - name: The name of the model used for fitting.\n",
    "            - params: The optimal parameters found for the model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 识别峰位\n",
    "        peak_indexs = get_peaks(y_data)\n",
    "        peaks_num = len(peak_indexs)\n",
    "        if peaks_num == 0:\n",
    "            return results\n",
    "        iter_num = iter_num if iter_num else peaks_num\n",
    "\n",
    "        # 计算最高峰位的相关信息\n",
    "        scale = len(y_data)\n",
    "        max_peak_index= np.argmax(y_data[peak_indexs])\n",
    "        max_intensity = y_data[peak_indexs[max_peak_index]]\n",
    "        center = peak_indexs[max_peak_index] / scale\n",
    "        _width_scipy=signal.peak_widths(y_data, [peak_indexs[max_peak_index]], rel_height=0.5)[0][0] / scale\n",
    "        width = _width_scipy if _width_scipy > 0.02 else 0.02\n",
    "\n",
    "        # 设置不同模型拟合函数和初猜值\n",
    "        tasks = []\n",
    "        for model in models:\n",
    "            initial_func_guess=[]\n",
    "            if model.__name__ in ['gauss','lorentz']:\n",
    "                initial_func_guess = [max_intensity,center,width]\n",
    "            elif model.__name__ == 'voigt':\n",
    "                initial_func_guess = [max_intensity/4, center, width-0.01, width/2-0.01]\n",
    "            elif model.__name__ == 'exp':\n",
    "                initial_func_guess = [1.0, -10.0, -0.01]\n",
    "            params = {\n",
    "                'fun': partial(loss, func=model),\n",
    "                'x0': initial_func_guess,\n",
    "                'args': (x_data, y_data)\n",
    "            }\n",
    "            tasks.append({'name': model.__name__, 'params': copy.deepcopy(params)})\n",
    "\n",
    "        # 并行加速运行拟合函数，并行失败，待研究\n",
    "        ## task_results = Parallel(n_jobs=-1)(delayed(minimize)(**task['params']) for task in tasks)\n",
    "        task_results=[minimize(**task['params']) for task in tasks]\n",
    "\n",
    "        # 过滤拟合失败的结果\n",
    "        task_results_filtered= [result for result in task_results if not math.isnan(result.fun)]\n",
    "\n",
    "        # 选择拟合最好的模型\n",
    "        optimal_fit_info = min(task_results_filtered, key=lambda x: x.fun)\n",
    "        optimal_index = task_results.index(optimal_fit_info)\n",
    "        optimal_params= optimal_fit_info.x\n",
    "        model_func = models[optimal_index]\n",
    "\n",
    "        # 保存当前拟合的最优模型参数\n",
    "        results.append({\n",
    "            'name': model_func.__name__,\n",
    "            'params': optimal_params,\n",
    "        })\n",
    "\n",
    "        # 初始数据减去拟合函数的值，生成新的待拟合数据\n",
    "        y_fit= model_func(x_data, *optimal_params)\n",
    "        y_new = y_data - y_fit\n",
    "\n",
    "        # 递归拟合上一步的残差, 直至iter_num == 0\n",
    "        iter_num -= 1\n",
    "        if iter_num != 0:\n",
    "            return iter_peaks(x_data, y_new, iter_num, results)\n",
    "        else:\n",
    "            return results\n",
    "    except Exception as e:\n",
    "        print(f'peak process error in the {iter_num} iteration: {e}')\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_source= P('./dataCheck/spectrum/UV统一格式').glob('**/*.xlsx')\n",
    "results:Any= Parallel(n_jobs=-1)(delayed(get_data_from_excel)(i) for i in p_source)\n",
    "_results:Any = copy.deepcopy(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4pkl ={}\n",
    "for i in results:\n",
    "    # 前处理数据\n",
    "    data = i['raw_arr'][:,1]\n",
    "    y_data, scaler= pre_process(data[::-1])\n",
    "    # scaler.inverse_transform(y_data.reshape(-1, 1)).reshape(-1)\n",
    "    x_data = np.linspace(0, 1, len(y_data))\n",
    "    \n",
    "    # 拿到峰值\n",
    "    try:\n",
    "        peaks_indices = get_peaks(y_data)\n",
    "        peaks_arr= np.zeros(401)\n",
    "        peaks_arr[peaks_indices] = np.array(y_data)[peaks_indices]\n",
    "        i['peaks_arr'] = peaks_arr\n",
    "        data4pkl[i['name']] = {'name':i['name'], 'raw_arr':y_data,'peaks_arr': peaks_arr.tolist()}\n",
    "    except Exception as e:\n",
    "        print('error:',i.keys(), e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data4pkl['B2']['raw_arr'])\n",
    "#data4pkl['B2'].model_dump()\n",
    "data4pkl['B2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 写入\n",
    "import pickle\n",
    "with open('uv_data.pkl', 'wb') as f:\n",
    "    pickle.dump(data4pkl, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 读取\n",
    "import pickle\n",
    "with open('uv_data_with_types.pkl', 'rb') as f:\n",
    "    data_pkl=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_arr: list[Any] = [data_pkl[i].peaks_arr for i in data_pkl]\n",
    "matrix = np.concatenate(all_arr).reshape(-1,401)\n",
    "data_matrix_sparse = csr_matrix(matrix)\n",
    "\n",
    "# 稀疏数组\n",
    "target_array_sparse = csr_matrix(data_pkl['B2'].peaks_arr)  # (1, 401)\n",
    "\n",
    "# 将稀疏矩阵转换为密集格式\n",
    "data_matrix_dense1 = data_matrix_sparse.toarray()\n",
    "target_array_dense1 = target_array_sparse.toarray()\n",
    "\n",
    "# 计算余弦相似度\n",
    "similarities = 1 - cdist(target_array_dense1, data_matrix_dense1, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "similarities.reshape(-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timetest(f):\n",
    "    def wrap(*args, **kwargs):\n",
    "        import time\n",
    "        start = time.time()\n",
    "        result = f(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        print(f\"Time taken: {end - start} seconds\")\n",
    "        return result\n",
    "    return wrap\n",
    "\n",
    "@timetest\n",
    "def f(a,b):\n",
    "    out = [i for i in a if i in b]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = set([f'str-{i}' for i in range(10000)])\n",
    "b = set([f'str-{i}' for i in range(0,10000,20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.0010004043579101562 seconds\n"
     ]
    }
   ],
   "source": [
    "out = f(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heap: [1, 10, 5]\n",
      "The minimum element: 1\n",
      "Item pushed and popped: 2\n",
      "List converted to Heap: [2, 10, 20, 15, 14, 21]\n",
      "Minimum element replaced: 2\n",
      "Heap after replacement: [10, 14, 20, 15, 100, 21]\n",
      "Three largest elements: [100, 21, 20]\n",
      "Three smallest elements: [10, 14, 15]\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "# 创建一个空堆\n",
    "heap = []\n",
    "\n",
    "# 向堆中添加元素\n",
    "heapq.heappush(heap, 10)\n",
    "heapq.heappush(heap, 1)\n",
    "heapq.heappush(heap, 5)\n",
    "\n",
    "# 查看堆中的元素\n",
    "print(\"Heap:\", heap)\n",
    "\n",
    "# 弹出最小的元素\n",
    "min_item = heapq.heappop(heap)\n",
    "print(\"The minimum element:\", min_item)\n",
    "\n",
    "# 添加一个元素，然后立即弹出最小的元素\n",
    "item = heapq.heappushpop(heap, 2)\n",
    "print(\"Item pushed and popped:\", item)\n",
    "\n",
    "# 将一个列表转换为堆\n",
    "list_for_heap = [20, 14, 2, 15, 10, 21]\n",
    "heapq.heapify(list_for_heap)\n",
    "print(\"List converted to Heap:\", list_for_heap)\n",
    "\n",
    "# 替换最小的元素\n",
    "min_item_replaced = heapq.heapreplace(list_for_heap, 100)\n",
    "print(\"Minimum element replaced:\", min_item_replaced)\n",
    "print(\"Heap after replacement:\", list_for_heap)\n",
    "\n",
    "# 获取最大的三个元素\n",
    "largest = heapq.nlargest(3, list_for_heap)\n",
    "print(\"Three largest elements:\", largest)\n",
    "\n",
    "# 获取最小的三个元素\n",
    "smallest = heapq.nsmallest(3, list_for_heap)\n",
    "print(\"Three smallest elements:\", smallest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trailingZeroes( n: int) :\n",
    "    p=1\n",
    "    ans=0\n",
    "    for i in range(1,n+1):\n",
    "        p=p*i\n",
    "        print(p)\n",
    "        if str(p)[-1]=='0':\n",
    "            ans+=1\n",
    "            p=int(p/10)\n",
    "    return ans        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "6\n",
      "24\n",
      "120\n",
      "72\n",
      "504\n",
      "4032\n",
      "36288\n",
      "362880\n",
      "399168\n",
      "4790016\n",
      "62270208\n",
      "871782912\n",
      "13076743680\n",
      "20922789888\n",
      "355687428096\n",
      "6402373705728\n",
      "121645100408832\n",
      "2432902008176640\n",
      "5109094217170944\n",
      "112400072777760768\n",
      "2585201673888497664\n",
      "62044840173323943936\n",
      "1551121004333098598400\n",
      "4032914611266056355840\n",
      "10888869450418352160768\n",
      "304888344611713860501504\n",
      "8841761993739701954543616\n",
      "265252859812191058636308480\n",
      "822283865417792249380470784\n",
      "26313083693369351980175065088\n",
      "868331761881188615345777147904\n",
      "29523279903960412921756423028736\n",
      "1033314796638614452261474806005760\n",
      "3719933267899011782369124412293120\n",
      "13763753091226344927831250027151360\n",
      "52302261746660113463947323544436736\n",
      "2039788208119744425093945618233032704\n",
      "81591528324789777003757824729321308160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trailingZeroes(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 生成一个随机的目标数组\n",
    "target_array = np.random.randint(0, 2, size=(401,))\n",
    "\n",
    "# 生成20000个随机的数组，放入一个大的数组中\n",
    "array_set = np.random.randint(0, 2, size=(2000000, 401))\n",
    "\n",
    "# 计算目标数组与每个数组的相似度（汉明距离）\n",
    "hamming_distances = np.sum(target_array != array_set, axis=1)\n",
    "\n",
    "# 找到最相似的数组的索引\n",
    "most_similar_index = np.argmin(hamming_distances)\n",
    "\n",
    "# 输出结果\n",
    "print(\"最相似的数组的索引：\", most_similar_index)\n",
    "print(\"最相似的数组：\", array_set[most_similar_index])\n",
    "print(\"汉明距离：\", hamming_distances[most_similar_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=np.array([[1,2,3,{}]])\n",
    "isinstance(s,np.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[2](0.3,*[0.24982069672131227, 0.5336658354114713, 0.26760417134639103])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(_results[4].values())[0][1:,1]\n",
    "\n",
    "# 前处理数据\n",
    "y_data, scaler= pre_process(data[::-1])\n",
    "# scaler.inverse_transform(y_data.reshape(-1, 1)).reshape(-1)\n",
    "x_data = np.linspace(0, 1, len(y_data))\n",
    "\n",
    "# 拟合\n",
    "result = iter_peaks(x_data, y_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = np.diff([])\n",
    "separators = np.where(diffs >= 2)[0] + 1\n",
    "print(separators)\n",
    "subarrays= np.split([], separators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subarrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义要拟合的函数列表\n",
    "class peak_funcs:\n",
    "    \n",
    "    @staticmethod\n",
    "    def exp(x, a, b, c):\n",
    "        return a * np.exp(b * (x-c))\n",
    "    \n",
    "    @staticmethod\n",
    "    def gauss(x, A, mu, sigma):\n",
    "        return A * np.exp(-(x - mu)**2 / (2 * sigma**2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def lorentz(x, A, mu, gamma):\n",
    "        return A / (1 + ((x - mu) / gamma)**2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def voigt(x, A, mu, sigma, gamma):\n",
    "        return A * voigt_profile(x - mu, sigma, gamma)\n",
    "\n",
    "models = [getattr(peak_funcs, i) for i in dir(peak_funcs) if not i.startswith('__')]\n",
    "\n",
    "def loss(params, x, y, func):\n",
    "    y_pred = func(x, *params)\n",
    "    loss_v = np.sum((y - y_pred) + np.maximum(0.001, 10*(y_pred - y)))\n",
    "    return loss_v\n",
    "\n",
    "tasks = []\n",
    "for model in models:\n",
    "    initial_func_guess = [1.0, -10.0, -0.01]\n",
    "    params = {\n",
    "        'fun': loss,\n",
    "        'x0': initial_func_guess,\n",
    "        'args':(x_data, y_data, model)\n",
    "    }\n",
    "    tasks.append({'name': model.__name__, 'params': copy.deepcopy(params)})\n",
    "# 并行加速运行拟合函数\n",
    "#task_results=[minimize(**task['params']) for task in tasks]\n",
    "task_results = Parallel(n_jobs=-1)(delayed(minimize)(**task['params']) for task in tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "import copy\n",
    "_results=copy.deepcopy(results)\n",
    "data = list(_results[13].values())[0][1:,1]\n",
    "y_data, scaler= pre_process(data[::-1])\n",
    "# scaler.inverse_transform(y_data.reshape(-1, 1)).reshape(-1)\n",
    "x_data = np.linspace(0, 1, len(y_data))\n",
    "peak_indexs: list[int] = get_peaks(y_data)\n",
    "plt.plot(x_data,y_data)\n",
    "plt.plot(x_data[peak_indexs], y_data[peak_indexs], \"x\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# 定义要拟合的函数列表\n",
    "models = [getattr(peak_funcs, i) for i in dir(peak_funcs) if not i.startswith('__')]\n",
    "\n",
    "# 定义损失函数\n",
    "def loss(params, x, y, func):\n",
    "    y_pred = func(x, *params)\n",
    "    loss_v = np.sum((y - y_pred) + np.maximum(0.001, 10*(y_pred - y)))\n",
    "    return loss_v\n",
    "\n",
    "# 前处理数据，获取峰值属性\n",
    "y_data, scaler= pre_process(data[::-1])\n",
    "# scaler.inverse_transform(y_data.reshape(-1, 1)).reshape(-1)\n",
    "x_data = np.linspace(0, 1, len(y_data))\n",
    "\n",
    "\n",
    "def iter_peaks(x_data, y_data, iter_num:int|None = None, results:list[dict] = []) -> list[dict]:\n",
    "    \n",
    "    try:\n",
    "        # 识别峰位\n",
    "        peak_indexs = get_peaks(y_data)\n",
    "        iter_num = iter_num if iter_num else len(peak_indexs)\n",
    "\n",
    "        # 计算最高峰位的相关信息\n",
    "        scale = len(y_data)\n",
    "        max_peak_index= np.argmax(y_data[peak_indexs])\n",
    "        max_intensity = y_data[peak_indexs[max_peak_index]]\n",
    "        center = peak_indexs[max_peak_index] / scale\n",
    "        _width_scipy=signal.peak_widths(y_data, [peak_indexs[max_peak_index]], rel_height=0.5)[0][0] / scale\n",
    "        width = _width_scipy if _width_scipy > 0.02 else 0.02\n",
    "\n",
    "        # 设置不同模型拟合函数和初猜值\n",
    "        tasks = []\n",
    "        for model in models:\n",
    "            if model.__name__ in ['gauss','lorentz']:\n",
    "                initial_func_guess = [max_intensity,center,width]\n",
    "            elif model.__name__ == 'voigt':\n",
    "                initial_func_guess = [max_intensity/4, center, width-0.01, width/2-0.01]\n",
    "            elif model.__name__ == 'exp':\n",
    "                initial_func_guess = [1.0, -10.0, -0.01]\n",
    "            params = {\n",
    "                'fun':partial(loss, func=model),\n",
    "                'x0':initial_func_guess,\n",
    "                'args':(x_data, y_data)\n",
    "            }\n",
    "            tasks.append({'name': model.__name__, 'params': params})\n",
    "\n",
    "        # 并行加速运行拟合函数\n",
    "        task_results = Parallel(n_jobs=-1)(delayed(minimize)(**task['params'])  for task in tasks)\n",
    "\n",
    "        # 过滤拟合失败的结果\n",
    "        task_results_filtered= [result for result in task_results if not math.isnan(result.fun)]\n",
    "\n",
    "        # 选择拟合最好的模型\n",
    "        optimal_fit_info = min(task_results_filtered, key=lambda x: x.fun)\n",
    "        optimal_index = task_results.index(optimal_fit_info)\n",
    "        optimal_params= optimal_fit_info.x\n",
    "        model_func = models[optimal_index]\n",
    "\n",
    "        # 初始数据减去拟合函数的值，生成新的待拟合数据\n",
    "        y_fit= model_func(x_data, *optimal_params)\n",
    "        y_new = y_data - y_fit\n",
    "\n",
    "        \n",
    "        iter_num -= 1\n",
    "    except Exception as e:\n",
    "        print(f'peak process error in the {iter_num} iteration: {e}')\n",
    "\n",
    "    if iter_num != 0:\n",
    "        # Recursively fit the remaining peaks\n",
    "        return iter_peaks(x_data, y_new, iter_num, results)\n",
    "    else:\n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 使用偏函数设定每个peak_fun的损失函数\n",
    "# loss_partials = [partial(loss, func=model) for model in models]\n",
    "\n",
    "# initial_func_guess = [1,0.2,0.5]\n",
    "# result = minimize(loss_partials[1], initial_func_guess, args=(np.linspace(0, 1, len(data_arr)), data_arr))\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_index=1\n",
    "y_pred=models[results_index](np.linspace(0, 1, len(y_data)),*results[results_index].x)#*fit_results[1]['params'])\n",
    "plt.plot(np.linspace(0, 1, len(y_data)),y_data)\n",
    "plt.plot(np.linspace(0, 1, len(y_data)), y_pred, \"r\")\n",
    "plt.show()\n",
    "print(models[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.linspace(0, 1, len(y_data))\n",
    "new_data = [y_data - models[i](x_data, *v.x) for i,v in enumerate(results)]\n",
    "errors=[sum(i) for i in new_data]\n",
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0, 1, len(y_data)), y_data-y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# 定义要拟合的函数\n",
    "def func(x, a, b, c):\n",
    "    return a * np.exp(-b * x) + c\n",
    "\n",
    "# 生成一些模拟数据\n",
    "x_data = np.linspace(0, 4, 50)\n",
    "y = func(x_data, 2.5, 1.3, 0.5)\n",
    "np.random.seed(1729)\n",
    "y_noise = 0.2 * np.random.normal(size=x_data.size)\n",
    "y_data = y + y_noise\n",
    "\n",
    "# 定义自定义的损失函数\n",
    "def custom_loss_function(params, x, y):\n",
    "    a, b, c = params\n",
    "    y_pred = func(x, a, b, c)\n",
    "    # 这里可以根据需要定义自己的损失函数，比如最大似然估计等\n",
    "    # 这里使用简单的平方损失作为示例\n",
    "    print(y_pred)\n",
    "    loss = np.sum((y - y_pred)**2)\n",
    "    return loss\n",
    "\n",
    "# 使用minimize进行拟合，传入自定义的损失函数\n",
    "initial_guess = [1.0, 1.0, 1.0]\n",
    "result = minimize(fun=custom_loss_function, x0=initial_guess, args=(x_data, y))\n",
    "\n",
    "# 输出拟合的参数\n",
    "print(result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.peak_prominences(data[:,1], [248, 269, 289, 305, 383],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_guess = []\n",
    "for i in peakind:\n",
    "    width=signal.peak_widths(data_arr, [i], rel_height=0.5)[0][0]\n",
    "    height=signal.peak_prominences(data_arr, [i])[0][0]\n",
    "    center = i\n",
    "    amplitude = height if height != 0 else 0.1\n",
    "    sigma = width/2.355 if width != 0 else 0.1\n",
    "    gamma = sigma/2 if width != 0 else 0.1\n",
    "    initial_guess.extend([center,amplitude,sigma,gamma])\n",
    "    print(center,amplitude,sigma,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_property,data[:,0][peakind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_half=signal.peak_widths(data[:,1], peakind, rel_height=0.5)\n",
    "plt.plot(data[:,1])\n",
    "plt.plot(peakind, data[:,1][peakind], \"x\")\n",
    "plt.hlines(*results_half[1:], color=\"C2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peakind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.peak_widths(data[:,1], [185], rel_height=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.peak_prominences(data[:,1],peakind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_full = signal.peak_widths(data[:,1], [185], rel_height=1)\n",
    "results_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:,0][peakind],len(peakind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.special import wofz\n",
    "\n",
    "# 定义Voigt函数\n",
    "def voigt(x, center, amplitude, sigma, gamma):\n",
    "    \"\"\"\n",
    "    Voigt函数是高斯函数和洛伦兹函数的卷积。\n",
    "    center: 峰的中心位置\n",
    "    amplitude: 峰的高度\n",
    "    sigma: 高斯分量的标准偏差\n",
    "    gamma: 洛伦兹分量的半宽度\n",
    "    \"\"\"\n",
    "    z = ((x-center) + 1j*gamma) / (sigma*np.sqrt(2))\n",
    "    return amplitude * np.real(wofz(z)) / (sigma*np.sqrt(2*np.pi))\n",
    "\n",
    "# 构建多个Voigt峰的组合函数\n",
    "def multiple_voigt(x, *params):\n",
    "    \"\"\"\n",
    "    params: 一个包含所有Voigt峰参数的列表，每个Voigt峰需要4个参数: center, amplitude, sigma, gamma\n",
    "    \"\"\"\n",
    "    y = np.zeros_like(x,dtype=np.float64)\n",
    "    for i in range(0, len(params), 4):\n",
    "        center = params[i]\n",
    "        amplitude = params[i+1]\n",
    "        sigma = params[i+2]\n",
    "        gamma = params[i+3]\n",
    "        y += voigt(x, center, amplitude, sigma, gamma)\n",
    "    return y\n",
    "\n",
    "# 假设的光谱数据及其噪声\n",
    "xdata = range(len(data_arr))\n",
    "ydata = data_arr\n",
    "\n",
    "# 初始猜测\n",
    "initial_guess = []\n",
    "for i in peakind:\n",
    "    width=signal.peak_widths(data_arr, [i], rel_height=0.5)[0][0]\n",
    "    height=signal.peak_prominences(data_arr, [i])[0][0]\n",
    "    amplitude = 20*data_arr[i]\n",
    "    sigma = width/2.355 if width != 0 else 10\n",
    "    gamma = sigma/16 if width != 0 else 1\n",
    "    initial_guess.extend([center,amplitude,sigma,gamma])\n",
    "    print(center,amplitude,sigma,gamma)\n",
    "\n",
    "# 执行拟合\n",
    "popt, pcov = curve_fit(multiple_voigt, xdata, ydata, p0=initial_guess, maxfev=10000,method='trf')\n",
    "\n",
    "# 输出最优拟合参数\n",
    "print(popt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_guess = []\n",
    "for i in peakind:\n",
    "    width=signal.peak_widths(data_arr, [i], rel_height=0.5)[0][0]\n",
    "    height=signal.peak_prominences(data_arr, [i])[0][0]\n",
    "    center = i\n",
    "    amplitude = 10*data_arr[i]*(1+height)\n",
    "    sigma = width if width != 0 else 10\n",
    "    gamma = sigma/100 if width != 0 else 0.02\n",
    "    initial_guess.extend([center,amplitude,sigma,gamma])\n",
    "    print(center,amplitude,sigma,gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xdata,ydata, \"c\")\n",
    "plt.plot(xdata,multiple_voigt(xdata,*initial_guess), \"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss=np.zeros_like([5,2])\n",
    "type(sss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logger_config\n",
    "\n",
    "logger = logger_config.get_logger(__name__)\n",
    "logger.debug('This is a debug message')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fullJustify(words: list[str], maxWidth: int) -> list[str]:\n",
    "    res=[]\n",
    "    cur_len=len(words[0])\n",
    "    line=words[0]\n",
    "    items_list=[words[0]]\n",
    "    for s in words[1:]:\n",
    "        n=len(s)\n",
    "        if cur_len + 1 + n <= maxWidth:\n",
    "            line = line + ' ' + s\n",
    "            cur_len = cur_len + 1 + n \n",
    "            items_list.append(s)\n",
    "        else:\n",
    "            extra_space=maxWidth - cur_len\n",
    "            if extra_space != 0:\n",
    "                if len(items_list) > 1:\n",
    "                    base_space = extra_space//(len(items_list)-1)\n",
    "                    left_space = extra_space%(len(items_list)-1)\n",
    "                    line = (' '*(base_space+2)).join(items_list[:left_space+1]) + ' '*(base_space+1)+ (' '*(base_space+1)).join(items_list[left_space+1:])\n",
    "                    print(len((' '*(base_space+2)).join(items_list[:left_space+1])),(' '*(base_space+2)).join(items_list[:left_space+1]),len((' '*(base_space+1)).join(items_list[left_space+1:])),(' '*(base_space+1)).join(items_list[left_space+1:]))\n",
    "                else:\n",
    "                    line = line + extra_space*' '\n",
    "            res.append(line)\n",
    "            cur_len=n\n",
    "            line=s\n",
    "            items_list=[s]\n",
    "    res.append(' '.join(line.strip().split()))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullJustify(words=[\"Science\",\"is\",\"what\",\"we\",\"understand\",\"well\",\"enough\",\"to\",\"explain\",\"to\",\"a\",\"computer.\",\"Art\",\"is\",\"everything\",\"else\",\"we\",\"do\"], maxWidth=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len('This    is    an')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from enumSmiles.utils import enumSetting,core,enum_atoms_smiles,enumData,molecule\n",
    "data=enumData(\n",
    "    core=core(\n",
    "        id=16,\n",
    "        smiles='C1(C=CC=C2)=C2C(C=CC=C3)=C3S1',\n",
    "        enumAtoms={\n",
    "            0: enumSetting(array=[10, 9, 8], range=[1, 3], connect2index=[0], keepSame2Index=[]), \n",
    "            2: enumSetting(array=[], range=[1, 3], connect2index=[2, 0], keepSame2Index=[])\n",
    "        },\n",
    "        enumBonds={}\n",
    "    ),\n",
    "    ligands=[\n",
    "        molecule(id=1, smiles='C1=CC1', atoms={0: [0]}, bonds={}), \n",
    "        molecule(id=2, smiles='C1CCC1', atoms={0: [0]}, bonds={}), \n",
    "        molecule(id=3, smiles='C1CCCC1', atoms={2: [2]}, bonds={})\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = await enum_atoms_smiles(data)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item=[1,2,3]\n",
    "d= {'a':item,'b':item}\n",
    "d['a'][2]=0\n",
    "item,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./test.txt', 'w') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line == '':\n",
    "            continue\n",
    "        f.write(line + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "stamp=datetime.now().timestamp()\n",
    "def get_date_from_timestamp(timestamp):\n",
    "    return datetime.strftime(timestamp,\"%Y%m%d\")\n",
    "get_date_from_timestamp(stamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_date_from_timestamp(timestamp):\n",
    "    date_time = datetime.fromtimestamp(timestamp)\n",
    "    return date_time.strftime(\"%Y%m%d\")\n",
    "\n",
    "# Get the current timestamp\n",
    "stamp = datetime.now().timestamp()\n",
    "\n",
    "# Get the date from the timestamp\n",
    "formatted_date = get_date_from_timestamp(stamp)\n",
    "print(formatted_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path as P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=P('./root/task')\n",
    "path.mkdir(mode=0o777, parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s='////// /s/d/2/'.strip(' /')\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = P('./my_text_file/ssd/sddds')\n",
    "s=p.absolute().as_posix()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "s=\"\"\"Job Id: 979305.mu01\n",
    "    Job_Name = g16\n",
    "    Job_Owner = root@mu01\n",
    "    resources_used.cput = 27:37:35\n",
    "    resources_used.mem = 20069212kb\n",
    "    resources_used.vmem = 36359272kb\n",
    "    resources_used.walltime = 00:52:23\n",
    "    job_state = R\n",
    "    queue = que\n",
    "    server = mu01\n",
    "    Checkpoint = u\n",
    "    ctime = Wed Jan 31 15:42:53 2024\n",
    "    Error_Path = mu01:/home/g16/HT/240124-RDF-0008/second/test/240124-RDF-0008\n",
    "       -S1-T1.gjf/g16.e979305\n",
    "    4exec_host = cu18/31+cu18/30+cu18/29+cu18/28+cu18/27+cu18/26+cu18/25+cu18/2\n",
    "        4+cu18/23+cu18/22+cu18/21+cu18/20+cu18/19+cu18/18+cu18/17+cu18/16+cu18\n",
    "        /15+cu18/14+cu18/13+cu18/12+cu18/11+cu18/10+cu18/9+cu18/8+cu18/7+cu18/\n",
    "        6+cu18/5+cu18/4+cu18/3+cu18/2+cu18/1+cu18/0\n",
    "    exec_port = 15003+15003+15003+15003+15003+15003+15003+15003+15003+15003+15\n",
    "        003+15003+15003+15003+15003+15003+15003+15003+15003+15003+15003+15003+\n",
    "        15003+15003+15003+15003+15003+15003+15003+15003+15003+15003\n",
    "\"\"\"\n",
    "ctime = re.search(r'ctime = (.*)', s).group(0)\n",
    "ctime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctime = re.search(r'ctime = (.*)', s).group(0)\n",
    "ctime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines[1].split()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "ctime_str = \"Wed Jan 31 15:42:53 2024\"\n",
    "format_string = \"%a %b %d %H:%M:%S %Y\"\n",
    "\n",
    "# 解析字符串为datetime对象\n",
    "ctime_datetime = datetime.strptime(ctime_str, format_string)\n",
    "\n",
    "print(ctime_datetime.timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a04f5d07b0747026a8fbcdf50b9443318e69b1b8bd6247d88bfadb4789282972"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
