{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame,Series\n",
    "import math\n",
    "import copy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import signal\n",
    "from numpy import ndarray\n",
    "from pathlib import Path as P\n",
    "from typing import Any\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import voigt_profile\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from scipy.optimize import minimize\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# 定义要拟合的函数列表\n",
    "class peak_funcs:\n",
    "    \n",
    "    @staticmethod\n",
    "    def exp(x, a, b, c):\n",
    "        return a * np.exp(b * (x-c))\n",
    "    \n",
    "    @staticmethod\n",
    "    def gauss(x, A, mu, sigma):\n",
    "        return A * np.exp(-(x - mu)**2 / (2 * sigma**2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def lorentz(x, A, mu, gamma):\n",
    "        return A / (1 + ((x - mu) / gamma)**2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def voigt(x, A, mu, sigma, gamma):\n",
    "        return A * voigt_profile(x - mu, sigma, gamma)\n",
    "\n",
    "models = [getattr(peak_funcs, i) for i in dir(peak_funcs) if not i.startswith('__')]\n",
    "\n",
    "# 定义损失函数\n",
    "def loss(params, x, y, func):\n",
    "    y_pred = func(x, *params)\n",
    "    diff = y - y_pred\n",
    "    diff = np.nan_to_num(diff)\n",
    "    diff = np.where(np.isnan(diff), 0, diff)\n",
    "    diff = np.where(np.isposinf(diff), 1.0, diff)\n",
    "    diff = np.where(np.isneginf(diff), -1.0, diff)\n",
    "    loss_v = np.sum(np.abs(diff) + np.maximum(0, 10*(diff)))\n",
    "    return loss_v\n",
    "\n",
    "# 读取excel\n",
    "def get_data_from_excel(file:P)-> dict:\n",
    "    filename: str= file.stem\n",
    "    df: DataFrame = pd.read_excel(file, header=None)\n",
    "    df.dropna(axis=0, how='all', inplace=True)\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    mask: DataFrame = df.applymap(lambda x: 'nm' == str(x).strip())    ## 在df中寻找值为字符串'nm'索引\n",
    "    mask_series: DataFrame | Series = mask.stack()                           ## 二维数据打平转化为series(row,col,value)\n",
    "    indices: list[Any] = mask_series[lambda x: x].index # type: ignore\n",
    "    if len(indices) > 1:\n",
    "        raise Exception(f'{filename}: 存在多个值为nm的单元格{indices}')\n",
    "    elif len(indices) == 0:\n",
    "        raise Exception(f'{filename}: 不存在值为nm的单元格')\n",
    "    else:\n",
    "        arr: ndarray = df.loc[indices[0][0]:,indices[0][1]:].to_numpy()\n",
    "        name: str = f'{filename}-{str(arr[0][1])}'\n",
    "        arr[0,1] = name\n",
    "        return {'name':filename, 'raw_arr': arr}\n",
    "\n",
    "# 数据前处理\n",
    "def pre_process(data:ndarray)-> ndarray:\n",
    "    scaler = MinMaxScaler()\n",
    "    arr_normalized = scaler.fit_transform(data.reshape(-1,1)).reshape(-1)\n",
    "    arr_normalized = signal.savgol_filter(arr_normalized, window_length=10, polyorder=2)\n",
    "    return arr_normalized, scaler # type: ignore\n",
    "\n",
    "## 寻找峰值\n",
    "def get_peaks(data:ndarray, threshold=10)-> list[int]:\n",
    "\n",
    "    peaks_normal: ndarray\n",
    "    _property:dict\n",
    "    peaks_normal, _property = signal.find_peaks(data, prominence=0.002, distance=10)\n",
    "    peaks_cwt: ndarray = signal.find_peaks_cwt(data, np.arange(1, 10), min_length=4, min_snr=1)\n",
    "    ## 合并去重,过滤低值\n",
    "    peaks_merged: list[Any] = sorted(list(set(peaks_normal.tolist() + peaks_cwt.tolist())))\n",
    "    peaks=[i for i in peaks_merged if data[i] > 0.05]\n",
    "    if len(peaks) == 0:\n",
    "        return []\n",
    "    ## 筛选主峰\n",
    "    diffs = np.diff(peaks)\n",
    "    separators = np.where(diffs >= threshold)[0] + 1\n",
    "    subarrays= np.split(peaks, separators)\n",
    "    peaks=[]\n",
    "    ## 密集区域稀疏化\n",
    "    for sub in subarrays:\n",
    "        if len(sub) == 1:\n",
    "            sub = sub[0]\n",
    "        else:\n",
    "            value_in_peaks_normal =np.array([i for i in sub if i in peaks_normal])\n",
    "            if len(value_in_peaks_normal) == 0:\n",
    "                sub = int(sub.mean())\n",
    "            else:\n",
    "                index = np.argmin(value_in_peaks_normal - sub.mean())\n",
    "                sub= value_in_peaks_normal[index]\n",
    "        peaks.append(sub)\n",
    "    # print('peaks:',peaks)\n",
    "    return peaks\n",
    "\n",
    "# 迭代寻找峰值主函数\n",
    "def iter_peaks(x_data, y_data, iter_num:int|None = None, results:list[dict] = []) -> list[dict]:\n",
    "    \"\"\"\n",
    "    find the best fitting model for each peak.\n",
    "\n",
    "    Args:\n",
    "        x_data: The x-axis data points.\n",
    "        y_data: The y-axis data points.\n",
    "        iter_num: 最大迭代次数 (optional).\n",
    "        results: 输出的结果 (optional).\n",
    "\n",
    "    Returns:\n",
    "        A list of fitting results, where each result contains:\n",
    "            - name: The name of the model used for fitting.\n",
    "            - params: The optimal parameters found for the model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 识别峰位\n",
    "        peak_indexs = get_peaks(y_data)\n",
    "        peaks_num = len(peak_indexs)\n",
    "        if peaks_num == 0:\n",
    "            return results\n",
    "        iter_num = iter_num if iter_num else peaks_num\n",
    "\n",
    "        # 计算最高峰位的相关信息\n",
    "        scale = len(y_data)\n",
    "        max_peak_index= np.argmax(y_data[peak_indexs])\n",
    "        max_intensity = y_data[peak_indexs[max_peak_index]]\n",
    "        center = peak_indexs[max_peak_index] / scale\n",
    "        _width_scipy=signal.peak_widths(y_data, [peak_indexs[max_peak_index]], rel_height=0.5)[0][0] / scale\n",
    "        width = _width_scipy if _width_scipy > 0.02 else 0.02\n",
    "\n",
    "        # 设置不同模型拟合函数和初猜值\n",
    "        tasks = []\n",
    "        for model in models:\n",
    "            initial_func_guess=[]\n",
    "            if model.__name__ in ['gauss','lorentz']:\n",
    "                initial_func_guess = [max_intensity,center,width]\n",
    "            elif model.__name__ == 'voigt':\n",
    "                initial_func_guess = [max_intensity/4, center, width-0.01, width/2-0.01]\n",
    "            elif model.__name__ == 'exp':\n",
    "                initial_func_guess = [1.0, -10.0, -0.01]\n",
    "            params = {\n",
    "                'fun': partial(loss, func=model),\n",
    "                'x0': initial_func_guess,\n",
    "                'args': (x_data, y_data)\n",
    "            }\n",
    "            tasks.append({'name': model.__name__, 'params': copy.deepcopy(params)})\n",
    "\n",
    "        # 并行加速运行拟合函数，并行失败，待研究\n",
    "        ## task_results = Parallel(n_jobs=-1)(delayed(minimize)(**task['params']) for task in tasks)\n",
    "        task_results=[minimize(**task['params']) for task in tasks]\n",
    "\n",
    "        # 过滤拟合失败的结果\n",
    "        task_results_filtered= [result for result in task_results if not math.isnan(result.fun)]\n",
    "\n",
    "        # 选择拟合最好的模型\n",
    "        optimal_fit_info = min(task_results_filtered, key=lambda x: x.fun)\n",
    "        optimal_index = task_results.index(optimal_fit_info)\n",
    "        optimal_params= optimal_fit_info.x\n",
    "        model_func = models[optimal_index]\n",
    "\n",
    "        # 保存当前拟合的最优模型参数\n",
    "        results.append({\n",
    "            'name': model_func.__name__,\n",
    "            'params': optimal_params,\n",
    "        })\n",
    "\n",
    "        # 初始数据减去拟合函数的值，生成新的待拟合数据\n",
    "        y_fit= model_func(x_data, *optimal_params)\n",
    "        y_new = y_data - y_fit\n",
    "\n",
    "        # 递归拟合上一步的残差, 直至iter_num == 0\n",
    "        iter_num -= 1\n",
    "        if iter_num != 0:\n",
    "            return iter_peaks(x_data, y_new, iter_num, results)\n",
    "        else:\n",
    "            return results\n",
    "    except Exception as e:\n",
    "        print(f'peak process error in the {iter_num} iteration: {e}')\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_source= P('./excel/UV统一格式').glob('**/*.xlsx')\n",
    "results:Any= Parallel(n_jobs=-1)(delayed(get_data_from_excel)(i) for i in p_source)\n",
    "_results:Any = copy.deepcopy(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in results:\n",
    "    # 前处理数据\n",
    "    data = i['raw_arr'][1:,1]\n",
    "    y_data, scaler= pre_process(data[::-1])\n",
    "    # scaler.inverse_transform(y_data.reshape(-1, 1)).reshape(-1)\n",
    "    x_data = np.linspace(0, 1, len(y_data))\n",
    "    \n",
    "    # 拿到峰值\n",
    "    try:\n",
    "        peaks_indices = get_peaks(y_data)\n",
    "        peaks_arr = np.zeros(401)\n",
    "        peaks_arr[peaks_indices] = y_data[peaks_indices]\n",
    "        i['peaks_arr'] = peaks_arr\n",
    "    except Exception as e:\n",
    "        print('error:',i.keys(), e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'B1',\n",
       " 'raw_arr': array([['nm', 'B1-T170719-12-1'],\n",
       "        [650, 0.041],\n",
       "        [649, 0.041],\n",
       "        [648, 0.041],\n",
       "        [647, 0.042],\n",
       "        [646, 0.041],\n",
       "        [645, 0.042],\n",
       "        [644, 0.041],\n",
       "        [643, 0.041],\n",
       "        [642, 0.042],\n",
       "        [641, 0.041],\n",
       "        [640, 0.041],\n",
       "        [639, 0.041],\n",
       "        [638, 0.042],\n",
       "        [637, 0.042],\n",
       "        [636, 0.042],\n",
       "        [635, 0.042],\n",
       "        [634, 0.042],\n",
       "        [633, 0.041],\n",
       "        [632, 0.041],\n",
       "        [631, 0.042],\n",
       "        [630, 0.042],\n",
       "        [629, 0.042],\n",
       "        [628, 0.042],\n",
       "        [627, 0.042],\n",
       "        [626, 0.042],\n",
       "        [625, 0.042],\n",
       "        [624, 0.042],\n",
       "        [623, 0.042],\n",
       "        [622, 0.042],\n",
       "        [621, 0.042],\n",
       "        [620, 0.042],\n",
       "        [619, 0.042],\n",
       "        [618, 0.041],\n",
       "        [617, 0.042],\n",
       "        [616, 0.042],\n",
       "        [615, 0.042],\n",
       "        [614, 0.042],\n",
       "        [613, 0.042],\n",
       "        [612, 0.042],\n",
       "        [611, 0.042],\n",
       "        [610, 0.042],\n",
       "        [609, 0.043],\n",
       "        [608, 0.042],\n",
       "        [607, 0.042],\n",
       "        [606, 0.042],\n",
       "        [605, 0.042],\n",
       "        [604, 0.042],\n",
       "        [603, 0.042],\n",
       "        [602, 0.043],\n",
       "        [601, 0.042],\n",
       "        [600, 0.042],\n",
       "        [599, 0.042],\n",
       "        [598, 0.042],\n",
       "        [597, 0.042],\n",
       "        [596, 0.042],\n",
       "        [595, 0.042],\n",
       "        [594, 0.042],\n",
       "        [593, 0.042],\n",
       "        [592, 0.042],\n",
       "        [591, 0.042],\n",
       "        [590, 0.042],\n",
       "        [589, 0.042],\n",
       "        [588, 0.042],\n",
       "        [587, 0.042],\n",
       "        [586, 0.042],\n",
       "        [585, 0.042],\n",
       "        [584, 0.042],\n",
       "        [583, 0.042],\n",
       "        [582, 0.042],\n",
       "        [581, 0.042],\n",
       "        [580, 0.042],\n",
       "        [579, 0.042],\n",
       "        [578, 0.042],\n",
       "        [577, 0.042],\n",
       "        [576, 0.042],\n",
       "        [575, 0.042],\n",
       "        [574, 0.042],\n",
       "        [573, 0.042],\n",
       "        [572, 0.042],\n",
       "        [571, 0.042],\n",
       "        [570, 0.042],\n",
       "        [569, 0.042],\n",
       "        [568, 0.041],\n",
       "        [567, 0.041],\n",
       "        [566, 0.041],\n",
       "        [565, 0.042],\n",
       "        [564, 0.042],\n",
       "        [563, 0.041],\n",
       "        [562, 0.041],\n",
       "        [561, 0.042],\n",
       "        [560, 0.041],\n",
       "        [559, 0.041],\n",
       "        [558, 0.041],\n",
       "        [557, 0.041],\n",
       "        [556, 0.041],\n",
       "        [555, 0.041],\n",
       "        [554, 0.041],\n",
       "        [553, 0.041],\n",
       "        [552, 0.041],\n",
       "        [551, 0.041],\n",
       "        [550, 0.04],\n",
       "        [549, 0.041],\n",
       "        [548, 0.04],\n",
       "        [547, 0.041],\n",
       "        [546, 0.04],\n",
       "        [545, 0.041],\n",
       "        [544, 0.04],\n",
       "        [543, 0.04],\n",
       "        [542, 0.04],\n",
       "        [541, 0.04],\n",
       "        [540, 0.04],\n",
       "        [539, 0.04],\n",
       "        [538, 0.04],\n",
       "        [537, 0.04],\n",
       "        [536, 0.04],\n",
       "        [535, 0.04],\n",
       "        [534, 0.04],\n",
       "        [533, 0.039],\n",
       "        [532, 0.039],\n",
       "        [531, 0.039],\n",
       "        [530, 0.039],\n",
       "        [529, 0.039],\n",
       "        [528, 0.039],\n",
       "        [527, 0.039],\n",
       "        [526, 0.038],\n",
       "        [525, 0.039],\n",
       "        [524, 0.038],\n",
       "        [523, 0.037],\n",
       "        [522, 0.038],\n",
       "        [521, 0.038],\n",
       "        [520, 0.037],\n",
       "        [519, 0.037],\n",
       "        [518, 0.037],\n",
       "        [517, 0.037],\n",
       "        [516, 0.037],\n",
       "        [515, 0.037],\n",
       "        [514, 0.037],\n",
       "        [513, 0.036],\n",
       "        [512, 0.037],\n",
       "        [511, 0.037],\n",
       "        [510, 0.036],\n",
       "        [509, 0.036],\n",
       "        [508, 0.036],\n",
       "        [507, 0.035],\n",
       "        [506, 0.036],\n",
       "        [505, 0.035],\n",
       "        [504, 0.036],\n",
       "        [503, 0.035],\n",
       "        [502, 0.035],\n",
       "        [501, 0.034],\n",
       "        [500, 0.034],\n",
       "        [499, 0.034],\n",
       "        [498, 0.034],\n",
       "        [497, 0.033],\n",
       "        [496, 0.034],\n",
       "        [495, 0.033],\n",
       "        [494, 0.033],\n",
       "        [493, 0.032],\n",
       "        [492, 0.033],\n",
       "        [491, 0.032],\n",
       "        [490, 0.032],\n",
       "        [489, 0.032],\n",
       "        [488, 0.032],\n",
       "        [487, 0.031],\n",
       "        [486, 0.031],\n",
       "        [485, 0.031],\n",
       "        [484, 0.031],\n",
       "        [483, 0.03],\n",
       "        [482, 0.03],\n",
       "        [481, 0.03],\n",
       "        [480, 0.03],\n",
       "        [479, 0.029],\n",
       "        [478, 0.029],\n",
       "        [477, 0.029],\n",
       "        [476, 0.029],\n",
       "        [475, 0.028],\n",
       "        [474, 0.028],\n",
       "        [473, 0.027],\n",
       "        [472, 0.027],\n",
       "        [471, 0.027],\n",
       "        [470, 0.027],\n",
       "        [469, 0.027],\n",
       "        [468, 0.026],\n",
       "        [467, 0.025],\n",
       "        [466, 0.026],\n",
       "        [465, 0.025],\n",
       "        [464, 0.025],\n",
       "        [463, 0.024],\n",
       "        [462, 0.024],\n",
       "        [461, 0.024],\n",
       "        [460, 0.023],\n",
       "        [459, 0.023],\n",
       "        [458, 0.022],\n",
       "        [457, 0.021],\n",
       "        [456, 0.022],\n",
       "        [455, 0.021],\n",
       "        [454, 0.02],\n",
       "        [453, 0.02],\n",
       "        [452, 0.02],\n",
       "        [451, 0.019],\n",
       "        [450, 0.018],\n",
       "        [449, 0.019],\n",
       "        [448, 0.018],\n",
       "        [447, 0.017],\n",
       "        [446, 0.016],\n",
       "        [445, 0.015],\n",
       "        [444, 0.016],\n",
       "        [443, 0.015],\n",
       "        [442, 0.015],\n",
       "        [441, 0.014],\n",
       "        [440, 0.013],\n",
       "        [439, 0.013],\n",
       "        [438, 0.012],\n",
       "        [437, 0.011],\n",
       "        [436, 0.011],\n",
       "        [435, 0.01],\n",
       "        [434, 0.01],\n",
       "        [433, 0.009],\n",
       "        [432, 0.009],\n",
       "        [431, 0.008],\n",
       "        [430, 0.007],\n",
       "        [429, 0.008],\n",
       "        [428, 0.007],\n",
       "        [427, 0.007],\n",
       "        [426, 0.008],\n",
       "        [425, 0.007],\n",
       "        [424, 0.009],\n",
       "        [423, 0.009],\n",
       "        [422, 0.013],\n",
       "        [421, 0.015],\n",
       "        [420, 0.019],\n",
       "        [419, 0.022],\n",
       "        [418, 0.028],\n",
       "        [417, 0.034],\n",
       "        [416, 0.041],\n",
       "        [415, 0.051],\n",
       "        [414, 0.061],\n",
       "        [413, 0.074],\n",
       "        [412, 0.089],\n",
       "        [411, 0.105],\n",
       "        [410, 0.123],\n",
       "        [409, 0.142],\n",
       "        [408, 0.165],\n",
       "        [407, 0.187],\n",
       "        [406, 0.21],\n",
       "        [405, 0.234],\n",
       "        [404, 0.261],\n",
       "        [403, 0.287],\n",
       "        [402, 0.315],\n",
       "        [401, 0.343],\n",
       "        [400, 0.372],\n",
       "        [399, 0.401],\n",
       "        [398, 0.431],\n",
       "        [397, 0.459],\n",
       "        [396, 0.487],\n",
       "        [395, 0.517],\n",
       "        [394, 0.545],\n",
       "        [393, 0.574],\n",
       "        [392, 0.6],\n",
       "        [391, 0.629],\n",
       "        [390, 0.656],\n",
       "        [389, 0.684],\n",
       "        [388, 0.709],\n",
       "        [387, 0.734],\n",
       "        [386, 0.759],\n",
       "        [385, 0.782],\n",
       "        [384, 0.803],\n",
       "        [383, 0.827],\n",
       "        [382, 0.846],\n",
       "        [381, 0.867],\n",
       "        [380, 0.887],\n",
       "        [379, 0.907],\n",
       "        [378, 0.924],\n",
       "        [377, 0.94],\n",
       "        [376, 0.955],\n",
       "        [375, 0.969],\n",
       "        [374, 0.983],\n",
       "        [373, 0.995],\n",
       "        [372, 1.006],\n",
       "        [371, 1.017],\n",
       "        [370, 1.026],\n",
       "        [369, 1.036],\n",
       "        [368, 1.043],\n",
       "        [367, 1.051],\n",
       "        [366, 1.055],\n",
       "        [365, 1.061],\n",
       "        [364, 1.066],\n",
       "        [363, 1.07],\n",
       "        [362, 1.073],\n",
       "        [361, 1.082],\n",
       "        [360, 1.086],\n",
       "        [359, 1.087],\n",
       "        [358, 1.087],\n",
       "        [357, 1.087],\n",
       "        [356, 1.085],\n",
       "        [355, 1.082],\n",
       "        [354, 1.077],\n",
       "        [353, 1.072],\n",
       "        [352, 1.068],\n",
       "        [351, 1.061],\n",
       "        [350, 1.052],\n",
       "        [349, 1.044],\n",
       "        [348, 1.035],\n",
       "        [347, 1.024],\n",
       "        [346, 1.013],\n",
       "        [345, 1.001],\n",
       "        [344, 0.989],\n",
       "        [343, 0.974],\n",
       "        [342, 0.959],\n",
       "        [341, 0.945],\n",
       "        [340, 0.928],\n",
       "        [339, 0.91],\n",
       "        [338, 0.893],\n",
       "        [337, 0.876],\n",
       "        [336, 0.857],\n",
       "        [335, 0.835],\n",
       "        [334, 0.817],\n",
       "        [333, 0.798],\n",
       "        [332, 0.777],\n",
       "        [331, 0.754],\n",
       "        [330, 0.732],\n",
       "        [329, 0.712],\n",
       "        [328, 0.69],\n",
       "        [327, 0.669],\n",
       "        [326, 0.647],\n",
       "        [325, 0.627],\n",
       "        [324, 0.606],\n",
       "        [323, 0.587],\n",
       "        [322, 0.567],\n",
       "        [321, 0.548],\n",
       "        [320, 0.53],\n",
       "        [319, 0.514],\n",
       "        [318, 0.5],\n",
       "        [317, 0.486],\n",
       "        [316, 0.472],\n",
       "        [315, 0.462],\n",
       "        [314, 0.451],\n",
       "        [313, 0.443],\n",
       "        [312, 0.435],\n",
       "        [311, 0.427],\n",
       "        [310, 0.423],\n",
       "        [309, 0.419],\n",
       "        [308, 0.415],\n",
       "        [307, 0.414],\n",
       "        [306, 0.411],\n",
       "        [305, 0.411],\n",
       "        [304, 0.411],\n",
       "        [303, 0.412],\n",
       "        [302, 0.414],\n",
       "        [301, 0.414],\n",
       "        [300, 0.416],\n",
       "        [299, 0.418],\n",
       "        [298, 0.421],\n",
       "        [297, 0.423],\n",
       "        [296, 0.426],\n",
       "        [295, 0.429],\n",
       "        [294, 0.431],\n",
       "        [293, 0.433],\n",
       "        [292, 0.435],\n",
       "        [291, 0.438],\n",
       "        [290, 0.44],\n",
       "        [289, 0.442],\n",
       "        [288, 0.444],\n",
       "        [287, 0.447],\n",
       "        [286, 0.447],\n",
       "        [285, 0.448],\n",
       "        [284, 0.448],\n",
       "        [283, 0.45],\n",
       "        [282, 0.452],\n",
       "        [281, 0.452],\n",
       "        [280, 0.454],\n",
       "        [279, 0.455],\n",
       "        [278, 0.457],\n",
       "        [277, 0.458],\n",
       "        [276, 0.462],\n",
       "        [275, 0.464],\n",
       "        [274, 0.467],\n",
       "        [273, 0.471],\n",
       "        [272, 0.475],\n",
       "        [271, 0.48],\n",
       "        [270, 0.485],\n",
       "        [269, 0.492],\n",
       "        [268, 0.499],\n",
       "        [267, 0.506],\n",
       "        [266, 0.515],\n",
       "        [265, 0.524],\n",
       "        [264, 0.534],\n",
       "        [263, 0.544],\n",
       "        [262, 0.553],\n",
       "        [261, 0.564],\n",
       "        [260, 0.576],\n",
       "        [259, 0.586],\n",
       "        [258, 0.596],\n",
       "        [257, 0.606],\n",
       "        [256, 0.615],\n",
       "        [255, 0.626],\n",
       "        [254, 0.63],\n",
       "        [253, 0.639],\n",
       "        [252, 0.645],\n",
       "        [251, 0.651],\n",
       "        [250, 0.654]], dtype=object),\n",
       " 'peaks_arr': array([0.        , 0.        , 0.        , 0.        , 0.57848485,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.40202546,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.00074074, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.87119792, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "matrix = np.concatenate([i['peaks_arr'] for i in results]).reshape(-1,401)\n",
    "data_matrix_sparse = csr_matrix(matrix)\n",
    "\n",
    "# 假设results[1]['peaks_arr']是你要比较的稀疏数组\n",
    "target_array_sparse = csr_matrix(results[1]['peaks_arr'])  # (1, 512)\n",
    "\n",
    "# 将稀疏矩阵转换为密集格式\n",
    "data_matrix_dense = data_matrix_sparse.toarray()\n",
    "target_array_dense = target_array_sparse.toarray()\n",
    "\n",
    "# 计算余弦相似度\n",
    "similarities = 1 - cdist(target_array_dense, data_matrix_dense, metric='cosine')\n",
    "\n",
    "# 定义一个函数，函数接收一个n:int，按照相似度，返回第n大相似度的索引. eg:找到最大相似的索引get_similar_index(0)\n",
    "def get_similar_index(n:int):\n",
    "    return np.argsort(similarities[0])[::-1][n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similar_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[1,2,3,4,{'na':2},9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, {'na': 2}, 9]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=[i for i in a]\n",
    "b[4]={'na':4}\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "389"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(similarities[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "389"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "389"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('uv_data.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最相似的数组的索引： 1804958\n",
      "最相似的数组： [1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0\n",
      " 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 1 1 0\n",
      " 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1\n",
      " 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 0 0 0 1\n",
      " 0 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 0\n",
      " 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 0\n",
      " 0 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 1\n",
      " 1 1 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 0 1 0\n",
      " 1 1 1 1 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 1\n",
      " 0 1 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0\n",
      " 1 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0]\n",
      "汉明距离： 150\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 生成一个随机的目标数组\n",
    "target_array = np.random.randint(0, 2, size=(401,))\n",
    "\n",
    "# 生成20000个随机的数组，放入一个大的数组中\n",
    "array_set = np.random.randint(0, 2, size=(2000000, 401))\n",
    "\n",
    "# 计算目标数组与每个数组的相似度（汉明距离）\n",
    "hamming_distances = np.sum(target_array != array_set, axis=1)\n",
    "\n",
    "# 找到最相似的数组的索引\n",
    "most_similar_index = np.argmin(hamming_distances)\n",
    "\n",
    "# 输出结果\n",
    "print(\"最相似的数组的索引：\", most_similar_index)\n",
    "print(\"最相似的数组：\", array_set[most_similar_index])\n",
    "print(\"汉明距离：\", hamming_distances[most_similar_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14174720502522295"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[2](0.3,*[0.24982069672131227, 0.5336658354114713, 0.26760417134639103])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peaks: [3, 51, 112, 132, 192, 205]\n",
      "peaks: [3]\n",
      "[{'name': 'gauss', 'params': array([0.97519626, 0.05155082, 0.22422108])}, {'name': 'voigt', 'params': array([0.54763846, 0.16586225, 0.16228214, 0.02205735])}, {'name': 'voigt', 'params': array([ 0.00216881, -0.05725889,  0.21595332, -0.68016123])}, {'name': 'voigt', 'params': array([0.35969971, 0.26019285, 0.06438009, 0.07597231])}, {'name': 'voigt', 'params': array([ 0.01323537, -0.0204016 ,  0.02903179, -0.01426286])}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chensq\\AppData\\Local\\Temp\\ipykernel_6544\\3244157405.py:138: PeakPropertyWarning: some peaks have a prominence of 0\n",
      "  _width_scipy=signal.peak_widths(y_data, [peak_indexs[max_peak_index]], rel_height=0.5)[0][0] / scale\n",
      "C:\\Users\\chensq\\AppData\\Local\\Temp\\ipykernel_6544\\3244157405.py:138: PeakPropertyWarning: some peaks have a width of 0\n",
      "  _width_scipy=signal.peak_widths(y_data, [peak_indexs[max_peak_index]], rel_height=0.5)[0][0] / scale\n"
     ]
    }
   ],
   "source": [
    "data = list(_results[4].values())[0][1:,1]\n",
    "\n",
    "# 前处理数据\n",
    "y_data, scaler= pre_process(data[::-1])\n",
    "# scaler.inverse_transform(y_data.reshape(-1, 1)).reshape(-1)\n",
    "x_data = np.linspace(0, 1, len(y_data))\n",
    "\n",
    "# 拟合\n",
    "result = iter_peaks(x_data, y_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'gauss', 'params': array([ 1.38751182, -0.1563219 ,  0.1787239 ])}, {'name': 'gauss', 'params': array([ 0.01334158,  0.6651137 , -0.14661715])}, {'name': 'voigt', 'params': array([ 0.04522934,  0.21267611,  0.05175714, -0.02124526])}, {'name': 'lorentz', 'params': array([0.40651891, 0.1495361 , 0.01006938])}, {'name': 'gauss', 'params': array([ 0.25193942,  0.25804819, -0.01586599])}]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "diffs = np.diff([])\n",
    "separators = np.where(diffs >= 2)[0] + 1\n",
    "print(separators)\n",
    "subarrays= np.split([], separators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([], dtype=float64)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subarrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义要拟合的函数列表\n",
    "class peak_funcs:\n",
    "    \n",
    "    @staticmethod\n",
    "    def exp(x, a, b, c):\n",
    "        return a * np.exp(b * (x-c))\n",
    "    \n",
    "    @staticmethod\n",
    "    def gauss(x, A, mu, sigma):\n",
    "        return A * np.exp(-(x - mu)**2 / (2 * sigma**2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def lorentz(x, A, mu, gamma):\n",
    "        return A / (1 + ((x - mu) / gamma)**2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def voigt(x, A, mu, sigma, gamma):\n",
    "        return A * voigt_profile(x - mu, sigma, gamma)\n",
    "\n",
    "models = [getattr(peak_funcs, i) for i in dir(peak_funcs) if not i.startswith('__')]\n",
    "\n",
    "def loss(params, x, y, func):\n",
    "    y_pred = func(x, *params)\n",
    "    loss_v = np.sum((y - y_pred) + np.maximum(0.001, 10*(y_pred - y)))\n",
    "    return loss_v\n",
    "\n",
    "tasks = []\n",
    "for model in models:\n",
    "    initial_func_guess = [1.0, -10.0, -0.01]\n",
    "    params = {\n",
    "        'fun': loss,\n",
    "        'x0': initial_func_guess,\n",
    "        'args':(x_data, y_data, model)\n",
    "    }\n",
    "    tasks.append({'name': model.__name__, 'params': copy.deepcopy(params)})\n",
    "# 并行加速运行拟合函数\n",
    "#task_results=[minimize(**task['params']) for task in tasks]\n",
    "task_results = Parallel(n_jobs=-1)(delayed(minimize)(**task['params']) for task in tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "import copy\n",
    "_results=copy.deepcopy(results)\n",
    "data = list(_results[13].values())[0][1:,1]\n",
    "y_data, scaler= pre_process(data[::-1])\n",
    "# scaler.inverse_transform(y_data.reshape(-1, 1)).reshape(-1)\n",
    "x_data = np.linspace(0, 1, len(y_data))\n",
    "peak_indexs: list[int] = get_peaks(y_data)\n",
    "plt.plot(x_data,y_data)\n",
    "plt.plot(x_data[peak_indexs], y_data[peak_indexs], \"x\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# 定义要拟合的函数列表\n",
    "models = [getattr(peak_funcs, i) for i in dir(peak_funcs) if not i.startswith('__')]\n",
    "\n",
    "# 定义损失函数\n",
    "def loss(params, x, y, func):\n",
    "    y_pred = func(x, *params)\n",
    "    loss_v = np.sum((y - y_pred) + np.maximum(0.001, 10*(y_pred - y)))\n",
    "    return loss_v\n",
    "\n",
    "# 前处理数据，获取峰值属性\n",
    "y_data, scaler= pre_process(data[::-1])\n",
    "# scaler.inverse_transform(y_data.reshape(-1, 1)).reshape(-1)\n",
    "x_data = np.linspace(0, 1, len(y_data))\n",
    "\n",
    "\n",
    "def iter_peaks(x_data, y_data, iter_num:int|None = None, results:list[dict] = []) -> list[dict]:\n",
    "    \n",
    "    try:\n",
    "        # 识别峰位\n",
    "        peak_indexs = get_peaks(y_data)\n",
    "        iter_num = iter_num if iter_num else len(peak_indexs)\n",
    "\n",
    "        # 计算最高峰位的相关信息\n",
    "        scale = len(y_data)\n",
    "        max_peak_index= np.argmax(y_data[peak_indexs])\n",
    "        max_intensity = y_data[peak_indexs[max_peak_index]]\n",
    "        center = peak_indexs[max_peak_index] / scale\n",
    "        _width_scipy=signal.peak_widths(y_data, [peak_indexs[max_peak_index]], rel_height=0.5)[0][0] / scale\n",
    "        width = _width_scipy if _width_scipy > 0.02 else 0.02\n",
    "\n",
    "        # 设置不同模型拟合函数和初猜值\n",
    "        tasks = []\n",
    "        for model in models:\n",
    "            if model.__name__ in ['gauss','lorentz']:\n",
    "                initial_func_guess = [max_intensity,center,width]\n",
    "            elif model.__name__ == 'voigt':\n",
    "                initial_func_guess = [max_intensity/4, center, width-0.01, width/2-0.01]\n",
    "            elif model.__name__ == 'exp':\n",
    "                initial_func_guess = [1.0, -10.0, -0.01]\n",
    "            params = {\n",
    "                'fun':partial(loss, func=model),\n",
    "                'x0':initial_func_guess,\n",
    "                'args':(x_data, y_data)\n",
    "            }\n",
    "            tasks.append({'name': model.__name__, 'params': params})\n",
    "\n",
    "        # 并行加速运行拟合函数\n",
    "        task_results = Parallel(n_jobs=-1)(delayed(minimize)(**task['params'])  for task in tasks)\n",
    "\n",
    "        # 过滤拟合失败的结果\n",
    "        task_results_filtered= [result for result in task_results if not math.isnan(result.fun)]\n",
    "\n",
    "        # 选择拟合最好的模型\n",
    "        optimal_fit_info = min(task_results_filtered, key=lambda x: x.fun)\n",
    "        optimal_index = task_results.index(optimal_fit_info)\n",
    "        optimal_params= optimal_fit_info.x\n",
    "        model_func = models[optimal_index]\n",
    "\n",
    "        # 初始数据减去拟合函数的值，生成新的待拟合数据\n",
    "        y_fit= model_func(x_data, *optimal_params)\n",
    "        y_new = y_data - y_fit\n",
    "\n",
    "        \n",
    "        iter_num -= 1\n",
    "    except Exception as e:\n",
    "        print(f'peak process error in the {iter_num} iteration: {e}')\n",
    "\n",
    "    if iter_num != 0:\n",
    "        # Recursively fit the remaining peaks\n",
    "        return iter_peaks(x_data, y_new, iter_num, results)\n",
    "    else:\n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 使用偏函数设定每个peak_fun的损失函数\n",
    "# loss_partials = [partial(loss, func=model) for model in models]\n",
    "\n",
    "# initial_func_guess = [1,0.2,0.5]\n",
    "# result = minimize(loss_partials[1], initial_func_guess, args=(np.linspace(0, 1, len(data_arr)), data_arr))\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_index=1\n",
    "y_pred=models[results_index](np.linspace(0, 1, len(y_data)),*results[results_index].x)#*fit_results[1]['params'])\n",
    "plt.plot(np.linspace(0, 1, len(y_data)),y_data)\n",
    "plt.plot(np.linspace(0, 1, len(y_data)), y_pred, \"r\")\n",
    "plt.show()\n",
    "print(models[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.linspace(0, 1, len(y_data))\n",
    "new_data = [y_data - models[i](x_data, *v.x) for i,v in enumerate(results)]\n",
    "errors=[sum(i) for i in new_data]\n",
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0, 1, len(y_data)), y_data-y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# 定义要拟合的函数\n",
    "def func(x, a, b, c):\n",
    "    return a * np.exp(-b * x) + c\n",
    "\n",
    "# 生成一些模拟数据\n",
    "x_data = np.linspace(0, 4, 50)\n",
    "y = func(x_data, 2.5, 1.3, 0.5)\n",
    "np.random.seed(1729)\n",
    "y_noise = 0.2 * np.random.normal(size=x_data.size)\n",
    "y_data = y + y_noise\n",
    "\n",
    "# 定义自定义的损失函数\n",
    "def custom_loss_function(params, x, y):\n",
    "    a, b, c = params\n",
    "    y_pred = func(x, a, b, c)\n",
    "    # 这里可以根据需要定义自己的损失函数，比如最大似然估计等\n",
    "    # 这里使用简单的平方损失作为示例\n",
    "    print(y_pred)\n",
    "    loss = np.sum((y - y_pred)**2)\n",
    "    return loss\n",
    "\n",
    "# 使用minimize进行拟合，传入自定义的损失函数\n",
    "initial_guess = [1.0, 1.0, 1.0]\n",
    "result = minimize(fun=custom_loss_function, x0=initial_guess, args=(x_data, y))\n",
    "\n",
    "# 输出拟合的参数\n",
    "print(result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.peak_prominences(data[:,1], [248, 269, 289, 305, 383],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_guess = []\n",
    "for i in peakind:\n",
    "    width=signal.peak_widths(data_arr, [i], rel_height=0.5)[0][0]\n",
    "    height=signal.peak_prominences(data_arr, [i])[0][0]\n",
    "    center = i\n",
    "    amplitude = height if height != 0 else 0.1\n",
    "    sigma = width/2.355 if width != 0 else 0.1\n",
    "    gamma = sigma/2 if width != 0 else 0.1\n",
    "    initial_guess.extend([center,amplitude,sigma,gamma])\n",
    "    print(center,amplitude,sigma,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_property,data[:,0][peakind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_half=signal.peak_widths(data[:,1], peakind, rel_height=0.5)\n",
    "plt.plot(data[:,1])\n",
    "plt.plot(peakind, data[:,1][peakind], \"x\")\n",
    "plt.hlines(*results_half[1:], color=\"C2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peakind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.peak_widths(data[:,1], [185], rel_height=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.peak_prominences(data[:,1],peakind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_full = signal.peak_widths(data[:,1], [185], rel_height=1)\n",
    "results_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:,0][peakind],len(peakind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.special import wofz\n",
    "\n",
    "# 定义Voigt函数\n",
    "def voigt(x, center, amplitude, sigma, gamma):\n",
    "    \"\"\"\n",
    "    Voigt函数是高斯函数和洛伦兹函数的卷积。\n",
    "    center: 峰的中心位置\n",
    "    amplitude: 峰的高度\n",
    "    sigma: 高斯分量的标准偏差\n",
    "    gamma: 洛伦兹分量的半宽度\n",
    "    \"\"\"\n",
    "    z = ((x-center) + 1j*gamma) / (sigma*np.sqrt(2))\n",
    "    return amplitude * np.real(wofz(z)) / (sigma*np.sqrt(2*np.pi))\n",
    "\n",
    "# 构建多个Voigt峰的组合函数\n",
    "def multiple_voigt(x, *params):\n",
    "    \"\"\"\n",
    "    params: 一个包含所有Voigt峰参数的列表，每个Voigt峰需要4个参数: center, amplitude, sigma, gamma\n",
    "    \"\"\"\n",
    "    y = np.zeros_like(x,dtype=np.float64)\n",
    "    for i in range(0, len(params), 4):\n",
    "        center = params[i]\n",
    "        amplitude = params[i+1]\n",
    "        sigma = params[i+2]\n",
    "        gamma = params[i+3]\n",
    "        y += voigt(x, center, amplitude, sigma, gamma)\n",
    "    return y\n",
    "\n",
    "# 假设的光谱数据及其噪声\n",
    "xdata = range(len(data_arr))\n",
    "ydata = data_arr\n",
    "\n",
    "# 初始猜测\n",
    "initial_guess = []\n",
    "for i in peakind:\n",
    "    width=signal.peak_widths(data_arr, [i], rel_height=0.5)[0][0]\n",
    "    height=signal.peak_prominences(data_arr, [i])[0][0]\n",
    "    amplitude = 20*data_arr[i]\n",
    "    sigma = width/2.355 if width != 0 else 10\n",
    "    gamma = sigma/16 if width != 0 else 1\n",
    "    initial_guess.extend([center,amplitude,sigma,gamma])\n",
    "    print(center,amplitude,sigma,gamma)\n",
    "\n",
    "# 执行拟合\n",
    "popt, pcov = curve_fit(multiple_voigt, xdata, ydata, p0=initial_guess, maxfev=10000,method='trf')\n",
    "\n",
    "# 输出最优拟合参数\n",
    "print(popt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_guess = []\n",
    "for i in peakind:\n",
    "    width=signal.peak_widths(data_arr, [i], rel_height=0.5)[0][0]\n",
    "    height=signal.peak_prominences(data_arr, [i])[0][0]\n",
    "    center = i\n",
    "    amplitude = 10*data_arr[i]*(1+height)\n",
    "    sigma = width if width != 0 else 10\n",
    "    gamma = sigma/100 if width != 0 else 0.02\n",
    "    initial_guess.extend([center,amplitude,sigma,gamma])\n",
    "    print(center,amplitude,sigma,gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xdata,ydata, \"c\")\n",
    "plt.plot(xdata,multiple_voigt(xdata,*initial_guess), \"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss=np.zeros_like([5,2])\n",
    "type(sss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logger_config\n",
    "\n",
    "logger = logger_config.get_logger(__name__)\n",
    "logger.debug('This is a debug message')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c:\\\\Users\\\\chensq\\\\Desktop\\\\webApp\\\\webdist.1.0\\\\backend01',\n",
       " 'F:\\\\program\\\\chemoffice\\\\ChemScript\\\\Lib',\n",
       " 'f:\\\\program\\\\miniconda\\\\python311.zip',\n",
       " 'f:\\\\program\\\\miniconda\\\\DLLs',\n",
       " 'f:\\\\program\\\\miniconda\\\\Lib',\n",
       " 'f:\\\\program\\\\miniconda',\n",
       " '',\n",
       " 'f:\\\\program\\\\miniconda\\\\Lib\\\\site-packages',\n",
       " 'f:\\\\program\\\\miniconda\\\\Lib\\\\site-packages\\\\win32',\n",
       " 'f:\\\\program\\\\miniconda\\\\Lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'f:\\\\program\\\\miniconda\\\\Lib\\\\site-packages\\\\Pythonwin']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fullJustify(words: list[str], maxWidth: int) -> list[str]:\n",
    "    res=[]\n",
    "    cur_len=len(words[0])\n",
    "    line=words[0]\n",
    "    items_list=[words[0]]\n",
    "    for s in words[1:]:\n",
    "        n=len(s)\n",
    "        if cur_len + 1 + n <= maxWidth:\n",
    "            line = line + ' ' + s\n",
    "            cur_len = cur_len + 1 + n \n",
    "            items_list.append(s)\n",
    "        else:\n",
    "            extra_space=maxWidth - cur_len\n",
    "            if extra_space != 0:\n",
    "                if len(items_list) > 1:\n",
    "                    base_space = extra_space//(len(items_list)-1)\n",
    "                    left_space = extra_space%(len(items_list)-1)\n",
    "                    line = (' '*(base_space+2)).join(items_list[:left_space+1]) + ' '*(base_space+1)+ (' '*(base_space+1)).join(items_list[left_space+1:])\n",
    "                    print(len((' '*(base_space+2)).join(items_list[:left_space+1])),(' '*(base_space+2)).join(items_list[:left_space+1]),len((' '*(base_space+1)).join(items_list[left_space+1:])),(' '*(base_space+1)).join(items_list[left_space+1:]))\n",
    "                else:\n",
    "                    line = line + extra_space*' '\n",
    "            res.append(line)\n",
    "            cur_len=n\n",
    "            line=s\n",
    "            items_list=[s]\n",
    "    res.append(' '.join(line.strip().split()))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 Science  is  what 2 we\n",
      "10 understand 4 well\n",
      "17 a  computer.  Art 2 is\n",
      "10 everything 8 else  we\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Science  is  what we',\n",
       " 'understand      well',\n",
       " 'enough to explain to',\n",
       " 'a  computer.  Art is',\n",
       " 'everything  else  we',\n",
       " 'do']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullJustify(words=[\"Science\",\"is\",\"what\",\"we\",\"understand\",\"well\",\"enough\",\"to\",\"explain\",\"to\",\"a\",\"computer.\",\"Art\",\"is\",\"everything\",\"else\",\"we\",\"do\"], maxWidth=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('This    is    an')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from enumSmiles.utils import enumSetting,core,enum_atoms_smiles,enumData,molecule\n",
    "data=enumData(\n",
    "    core=core(\n",
    "        id=16,\n",
    "        smiles='C1(C=CC=C2)=C2C(C=CC=C3)=C3S1',\n",
    "        enumAtoms={\n",
    "            0: enumSetting(array=[10, 9, 8], range=[1, 3], connect2index=[0], keepSame2Index=[]), \n",
    "            2: enumSetting(array=[], range=[1, 3], connect2index=[2, 0], keepSame2Index=[])\n",
    "        },\n",
    "        enumBonds={}\n",
    "    ),\n",
    "    ligands=[\n",
    "        molecule(id=1, smiles='C1=CC1', atoms={0: [0]}, bonds={}), \n",
    "        molecule(id=2, smiles='C1CCC1', atoms={0: [0]}, bonds={}), \n",
    "        molecule(id=3, smiles='C1CCCC1', atoms={2: [2]}, bonds={})\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'atoms': {0: [['*C1=CC1', 4], ['*C1CCC1', 5]], 2: [['*C1CCCC1', 6]]}, 'bonds': {}}\n",
      "{0: enumSetting(array=[], range=[1, 3], connect2index=[0], keepSame2Index=[]), 2: enumSetting(array=[], range=[1, 3], connect2index=[2, 0], keepSame2Index=[])}\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = await enum_atoms_smiles(data)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item=[1,2,3]\n",
    "d= {'a':item,'b':9}\n",
    "del d['a']\n",
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a04f5d07b0747026a8fbcdf50b9443318e69b1b8bd6247d88bfadb4789282972"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
