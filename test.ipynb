{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame,Series\n",
    "import math\n",
    "import copy\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import signal\n",
    "from numpy import ndarray\n",
    "from pathlib import Path as P\n",
    "from typing import Any\n",
    "from typing import Generator\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import voigt_profile\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from scipy.optimize import minimize\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# 定义要拟合的函数列表\n",
    "class peak_funcs:\n",
    "    \n",
    "    @staticmethod\n",
    "    def exp(x, a, b, c):\n",
    "        return a * np.exp(b * (x-c))\n",
    "    \n",
    "    @staticmethod\n",
    "    def gauss(x, A, mu, sigma):\n",
    "        return A * np.exp(-(x - mu)**2 / (2 * sigma**2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def lorentz(x, A, mu, gamma):\n",
    "        return A / (1 + ((x - mu) / gamma)**2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def voigt(x, A, mu, sigma, gamma):\n",
    "        return A * voigt_profile(x - mu, sigma, gamma)\n",
    "\n",
    "models = [getattr(peak_funcs, i) for i in dir(peak_funcs) if not i.startswith('__')]\n",
    "\n",
    "# 定义损失函数\n",
    "def loss(params, x, y, func):\n",
    "    y_pred = func(x, *params)\n",
    "    loss_v = np.sum((y - y_pred) + np.maximum(0.001, 10*(y_pred - y)))\n",
    "    return loss_v\n",
    "\n",
    "# 读取excel\n",
    "def get_data_from_excel(file:P)-> dict[str, ndarray]:\n",
    "    filename: str= file.stem\n",
    "    df: DataFrame = pd.read_excel(file, header=None)\n",
    "    df.dropna(axis=0, how='all', inplace=True)\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    mask: DataFrame = df.applymap(lambda x: 'nm' == str(x).strip())    ## 在df中寻找值为字符串'nm'索引\n",
    "    mask_series: DataFrame | Series = mask.stack()                           ## 二维数据打平转化为series(row,col,value)\n",
    "    indices: list[Any] = mask_series[lambda x: x].index\n",
    "    if len(indices) > 1:\n",
    "        raise Exception(f'{filename}: 存在多个值为nm的单元格{indices}')\n",
    "    elif len(indices) == 0:\n",
    "        raise Exception(f'{filename}: 不存在值为nm的单元格')\n",
    "    else:\n",
    "        arr: ndarray = df.loc[indices[0][0]:,indices[0][1]:].to_numpy()\n",
    "        name: str = f'{filename}-{str(arr[0][1])}'\n",
    "        arr[0,1] = name\n",
    "        return {filename: arr}\n",
    "\n",
    "# 数据前处理\n",
    "def pre_process(data:ndarray)-> ndarray:\n",
    "    scaler = MinMaxScaler()\n",
    "    arr_normalized = scaler.fit_transform(data.reshape(-1,1)).reshape(-1)\n",
    "    arr_normalized = signal.savgol_filter(arr_normalized, window_length=10, polyorder=2)\n",
    "    return arr_normalized, scaler\n",
    "\n",
    "## 寻找峰值\n",
    "def get_peaks(data:ndarray, threshold=10)-> list[int]:\n",
    "\n",
    "    peaks_normal: ndarray\n",
    "    _property:dict\n",
    "    peaks_normal, _property = signal.find_peaks(data, prominence=0.002, distance=10)\n",
    "    peaks_cwt: ndarray = signal.find_peaks_cwt(data, np.arange(1, 10), min_length=4, min_snr=1)\n",
    "    ## 合并去重,过滤低值\n",
    "    peaks_merged: list[Any] = sorted(list(set(peaks_normal.tolist() + peaks_cwt.tolist())))\n",
    "    peaks=[i for i in peaks_merged if data[i] > 0.05]\n",
    "    ## 筛选主峰\n",
    "    diffs = np.diff(peaks)\n",
    "    separators = np.where(diffs >= threshold)[0] + 1\n",
    "    subarrays= np.split(peaks, separators)\n",
    "    peaks=[]\n",
    "    ## 密集区域稀疏化\n",
    "    for sub in subarrays:\n",
    "        if len(sub) == 1:\n",
    "            sub = sub[0]\n",
    "        else:\n",
    "            value_in_peaks_normal =np.array([i for i in sub if i in peaks_normal])\n",
    "            if len(value_in_peaks_normal) == 0:\n",
    "                sub = sub.mean()\n",
    "            else:\n",
    "                index = np.argmin(value_in_peaks_normal - sub.mean())\n",
    "                sub= value_in_peaks_normal[index]\n",
    "        peaks.append(sub)\n",
    "    print('peaks:',peaks)\n",
    "    return peaks\n",
    "\n",
    "# 迭代寻找峰值主函数\n",
    "def iter_peaks(x_data, y_data, iter_num:int|None = None, results:list[dict] = []) -> list[dict]:\n",
    "    \"\"\"\n",
    "    find the best fitting model for each peak.\n",
    "\n",
    "    Args:\n",
    "        x_data: The x-axis data points.\n",
    "        y_data: The y-axis data points.\n",
    "        iter_num: 最大迭代次数 (optional).\n",
    "        results: 输出的结果 (optional).\n",
    "\n",
    "    Returns:\n",
    "        A list of fitting results, where each result contains:\n",
    "            - name: The name of the model used for fitting.\n",
    "            - params: The optimal parameters found for the model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 识别峰位\n",
    "        peak_indexs = get_peaks(y_data)\n",
    "        iter_num = iter_num if iter_num else len(peak_indexs)\n",
    "\n",
    "        # 计算最高峰位的相关信息\n",
    "        scale = len(y_data)\n",
    "        max_peak_index= np.argmax(y_data[peak_indexs])\n",
    "        max_intensity = y_data[peak_indexs[max_peak_index]]\n",
    "        center = peak_indexs[max_peak_index] / scale\n",
    "        _width_scipy=signal.peak_widths(y_data, [peak_indexs[max_peak_index]], rel_height=0.5)[0][0] / scale\n",
    "        width = _width_scipy if _width_scipy > 0.02 else 0.02\n",
    "\n",
    "        # 设置不同模型拟合函数和初猜值\n",
    "        tasks = []\n",
    "        for model in models:\n",
    "            initial_func_guess=[]\n",
    "            if model.__name__ in ['gauss','lorentz']:\n",
    "                initial_func_guess = [max_intensity,center,width]\n",
    "            elif model.__name__ == 'voigt':\n",
    "                initial_func_guess = [max_intensity/4, center, width-0.01, width/2-0.01]\n",
    "            elif model.__name__ == 'exp':\n",
    "                initial_func_guess = [1.0, -10.0, -0.01]\n",
    "            params = {\n",
    "                'fun': partial(loss, func=model),\n",
    "                'x0': initial_func_guess,\n",
    "                'args': (x_data, y_data)\n",
    "            }\n",
    "            tasks.append({'name': model.__name__, 'params': copy.deepcopy(params)})\n",
    "\n",
    "        # 并行加速运行拟合函数，并行失败，待研究\n",
    "        ## task_results = Parallel(n_jobs=-1)(delayed(minimize)(**task['params']) for task in tasks)\n",
    "        task_results=[minimize(**task['params']) for task in tasks]\n",
    "\n",
    "        # 过滤拟合失败的结果\n",
    "        task_results_filtered= [result for result in task_results if not math.isnan(result.fun)]\n",
    "\n",
    "        # 选择拟合最好的模型\n",
    "        optimal_fit_info = min(task_results_filtered, key=lambda x: x.fun)\n",
    "        optimal_index = task_results.index(optimal_fit_info)\n",
    "        optimal_params= optimal_fit_info.x\n",
    "        model_func = models[optimal_index]\n",
    "\n",
    "        # 保存当前拟合的最优模型参数\n",
    "        results.append({\n",
    "            'name': model_func.__name__,\n",
    "            'params': optimal_params,\n",
    "        })\n",
    "\n",
    "        # 初始数据减去拟合函数的值，生成新的待拟合数据\n",
    "        y_fit= model_func(x_data, *optimal_params)\n",
    "        y_new = y_data - y_fit\n",
    "\n",
    "        # 递归拟合上一步的残差, 直至iter_num == 0\n",
    "        iter_num -= 1\n",
    "        if iter_num != 0:\n",
    "            return iter_peaks(x_data, y_new, iter_num, results)\n",
    "        else:\n",
    "            return results\n",
    "    except Exception as e:\n",
    "        print(f'peak process error in the {iter_num} iteration: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_source: Generator[P, None, None] = P('./excel/UV统一格式').glob('**/*.xlsx')\n",
    "results: list | None = Parallel(n_jobs=-1)(delayed(get_data_from_excel)(i) for i in p_source)\n",
    "_results=copy.deepcopy(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14174720502522295"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[2](0.3,*[0.24982069672131227, 0.5336658354114713, 0.26760417134639103])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peaks: [4, 31, 60, 75, 95]\n",
      "peaks: [29, 60, 98]\n",
      "peaks: [29, 60, 98]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chensq\\AppData\\Local\\Temp\\ipykernel_13016\\2675203447.py:126: PeakPropertyWarning: some peaks have a prominence of 0\n",
      "  _width_scipy=signal.peak_widths(y_data, [peak_indexs[max_peak_index]], rel_height=0.5)[0][0] / scale\n",
      "C:\\Users\\chensq\\AppData\\Local\\Temp\\ipykernel_13016\\2675203447.py:126: PeakPropertyWarning: some peaks have a width of 0\n",
      "  _width_scipy=signal.peak_widths(y_data, [peak_indexs[max_peak_index]], rel_height=0.5)[0][0] / scale\n",
      "C:\\Users\\chensq\\AppData\\Local\\Temp\\ipykernel_13016\\2675203447.py:44: RuntimeWarning: invalid value encountered in add\n",
      "  loss_v = np.sum((y - y_pred) + np.maximum(0.001, 10*(y_pred - y)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peaks: [29, 60, 104]\n",
      "peaks: [29, 49, 104]\n",
      "[{'name': 'gauss', 'params': array([ 1.38751182, -0.1563219 ,  0.1787239 ])}, {'name': 'gauss', 'params': array([ 0.01334158,  0.6651137 , -0.14661715])}, {'name': 'voigt', 'params': array([ 0.04522934,  0.21267611,  0.05175714, -0.02124526])}, {'name': 'lorentz', 'params': array([0.40651891, 0.1495361 , 0.01006938])}, {'name': 'gauss', 'params': array([ 0.25193942,  0.25804819, -0.01586599])}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chensq\\AppData\\Local\\Temp\\ipykernel_13016\\2675203447.py:44: RuntimeWarning: invalid value encountered in add\n",
      "  loss_v = np.sum((y - y_pred) + np.maximum(0.001, 10*(y_pred - y)))\n",
      "C:\\Users\\chensq\\AppData\\Local\\Temp\\ipykernel_13016\\2675203447.py:44: RuntimeWarning: overflow encountered in multiply\n",
      "  loss_v = np.sum((y - y_pred) + np.maximum(0.001, 10*(y_pred - y)))\n",
      "C:\\Users\\chensq\\AppData\\Local\\Temp\\ipykernel_13016\\2675203447.py:44: RuntimeWarning: invalid value encountered in add\n",
      "  loss_v = np.sum((y - y_pred) + np.maximum(0.001, 10*(y_pred - y)))\n",
      "C:\\Users\\chensq\\AppData\\Local\\Temp\\ipykernel_13016\\2675203447.py:44: RuntimeWarning: invalid value encountered in add\n",
      "  loss_v = np.sum((y - y_pred) + np.maximum(0.001, 10*(y_pred - y)))\n"
     ]
    }
   ],
   "source": [
    "data = list(_results[5].values())[0][1:,1]\n",
    "\n",
    "# 前处理数据\n",
    "y_data, scaler= pre_process(data[::-1])\n",
    "# scaler.inverse_transform(y_data.reshape(-1, 1)).reshape(-1)\n",
    "x_data = np.linspace(0, 1, len(y_data))\n",
    "\n",
    "# 拟合\n",
    "result = iter_peaks(x_data, y_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义要拟合的函数列表\n",
    "class peak_funcs:\n",
    "    \n",
    "    @staticmethod\n",
    "    def exp(x, a, b, c):\n",
    "        return a * np.exp(b * (x-c))\n",
    "    \n",
    "    @staticmethod\n",
    "    def gauss(x, A, mu, sigma):\n",
    "        return A * np.exp(-(x - mu)**2 / (2 * sigma**2))\n",
    "    \n",
    "    @staticmethod\n",
    "    def lorentz(x, A, mu, gamma):\n",
    "        return A / (1 + ((x - mu) / gamma)**2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def voigt(x, A, mu, sigma, gamma):\n",
    "        return A * voigt_profile(x - mu, sigma, gamma)\n",
    "\n",
    "models = [getattr(peak_funcs, i) for i in dir(peak_funcs) if not i.startswith('__')]\n",
    "\n",
    "def loss(params, x, y, func):\n",
    "    y_pred = func(x, *params)\n",
    "    loss_v = np.sum((y - y_pred) + np.maximum(0.001, 10*(y_pred - y)))\n",
    "    return loss_v\n",
    "\n",
    "tasks = []\n",
    "for model in models:\n",
    "    initial_func_guess = [1.0, -10.0, -0.01]\n",
    "    params = {\n",
    "        'fun': loss,\n",
    "        'x0': initial_func_guess,\n",
    "        'args':(x_data, y_data, model)\n",
    "    }\n",
    "    tasks.append({'name': model.__name__, 'params': copy.deepcopy(params)})\n",
    "# 并行加速运行拟合函数\n",
    "#task_results=[minimize(**task['params']) for task in tasks]\n",
    "task_results = Parallel(n_jobs=-1)(delayed(minimize)(**task['params']) for task in tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "import copy\n",
    "_results=copy.deepcopy(results)\n",
    "data = list(_results[13].values())[0][1:,1]\n",
    "y_data, scaler= pre_process(data[::-1])\n",
    "# scaler.inverse_transform(y_data.reshape(-1, 1)).reshape(-1)\n",
    "x_data = np.linspace(0, 1, len(y_data))\n",
    "peak_indexs: list[int] = get_peaks(y_data)\n",
    "plt.plot(x_data,y_data)\n",
    "plt.plot(x_data[peak_indexs], y_data[peak_indexs], \"x\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# 定义要拟合的函数列表\n",
    "models = [getattr(peak_funcs, i) for i in dir(peak_funcs) if not i.startswith('__')]\n",
    "\n",
    "# 定义损失函数\n",
    "def loss(params, x, y, func):\n",
    "    y_pred = func(x, *params)\n",
    "    loss_v = np.sum((y - y_pred) + np.maximum(0.001, 10*(y_pred - y)))\n",
    "    return loss_v\n",
    "\n",
    "# 前处理数据，获取峰值属性\n",
    "y_data, scaler= pre_process(data[::-1])\n",
    "# scaler.inverse_transform(y_data.reshape(-1, 1)).reshape(-1)\n",
    "x_data = np.linspace(0, 1, len(y_data))\n",
    "\n",
    "\n",
    "def iter_peaks(x_data, y_data, iter_num:int|None = None, results:list[dict] = []) -> list[dict]:\n",
    "    \n",
    "    try:\n",
    "        # 识别峰位\n",
    "        peak_indexs = get_peaks(y_data)\n",
    "        iter_num = iter_num if iter_num else len(peak_indexs)\n",
    "\n",
    "        # 计算最高峰位的相关信息\n",
    "        scale = len(y_data)\n",
    "        max_peak_index= np.argmax(y_data[peak_indexs])\n",
    "        max_intensity = y_data[peak_indexs[max_peak_index]]\n",
    "        center = peak_indexs[max_peak_index] / scale\n",
    "        _width_scipy=signal.peak_widths(y_data, [peak_indexs[max_peak_index]], rel_height=0.5)[0][0] / scale\n",
    "        width = _width_scipy if _width_scipy > 0.02 else 0.02\n",
    "\n",
    "        # 设置不同模型拟合函数和初猜值\n",
    "        tasks = []\n",
    "        for model in models:\n",
    "            if model.__name__ in ['gauss','lorentz']:\n",
    "                initial_func_guess = [max_intensity,center,width]\n",
    "            elif model.__name__ == 'voigt':\n",
    "                initial_func_guess = [max_intensity/4, center, width-0.01, width/2-0.01]\n",
    "            elif model.__name__ == 'exp':\n",
    "                initial_func_guess = [1.0, -10.0, -0.01]\n",
    "            params = {\n",
    "                'fun':partial(loss, func=model),\n",
    "                'x0':initial_func_guess,\n",
    "                'args':(x_data, y_data)\n",
    "            }\n",
    "            tasks.append({'name': model.__name__, 'params': params})\n",
    "\n",
    "        # 并行加速运行拟合函数\n",
    "        task_results = Parallel(n_jobs=-1)(delayed(minimize)(**task['params'])  for task in tasks)\n",
    "\n",
    "        # 过滤拟合失败的结果\n",
    "        task_results_filtered= [result for result in task_results if not math.isnan(result.fun)]\n",
    "\n",
    "        # 选择拟合最好的模型\n",
    "        optimal_fit_info = min(task_results_filtered, key=lambda x: x.fun)\n",
    "        optimal_index = task_results.index(optimal_fit_info)\n",
    "        optimal_params= optimal_fit_info.x\n",
    "        model_func = models[optimal_index]\n",
    "\n",
    "        # 初始数据减去拟合函数的值，生成新的待拟合数据\n",
    "        y_fit= model_func(x_data, *optimal_params)\n",
    "        y_new = y_data - y_fit\n",
    "\n",
    "        \n",
    "        iter_num -= 1\n",
    "    except Exception as e:\n",
    "        print(f'peak process error in the {iter_num} iteration: {e}')\n",
    "\n",
    "    if iter_num != 0:\n",
    "        # Recursively fit the remaining peaks\n",
    "        return iter_peaks(x_data, y_new, iter_num, results)\n",
    "    else:\n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 使用偏函数设定每个peak_fun的损失函数\n",
    "# loss_partials = [partial(loss, func=model) for model in models]\n",
    "\n",
    "# initial_func_guess = [1,0.2,0.5]\n",
    "# result = minimize(loss_partials[1], initial_func_guess, args=(np.linspace(0, 1, len(data_arr)), data_arr))\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_index=1\n",
    "y_pred=models[results_index](np.linspace(0, 1, len(y_data)),*results[results_index].x)#*fit_results[1]['params'])\n",
    "plt.plot(np.linspace(0, 1, len(y_data)),y_data)\n",
    "plt.plot(np.linspace(0, 1, len(y_data)), y_pred, \"r\")\n",
    "plt.show()\n",
    "print(models[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.linspace(0, 1, len(y_data))\n",
    "new_data = [y_data - models[i](x_data, *v.x) for i,v in enumerate(results)]\n",
    "errors=[sum(i) for i in new_data]\n",
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0, 1, len(y_data)), y_data-y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# 定义要拟合的函数\n",
    "def func(x, a, b, c):\n",
    "    return a * np.exp(-b * x) + c\n",
    "\n",
    "# 生成一些模拟数据\n",
    "x_data = np.linspace(0, 4, 50)\n",
    "y = func(x_data, 2.5, 1.3, 0.5)\n",
    "np.random.seed(1729)\n",
    "y_noise = 0.2 * np.random.normal(size=x_data.size)\n",
    "y_data = y + y_noise\n",
    "\n",
    "# 定义自定义的损失函数\n",
    "def custom_loss_function(params, x, y):\n",
    "    a, b, c = params\n",
    "    y_pred = func(x, a, b, c)\n",
    "    # 这里可以根据需要定义自己的损失函数，比如最大似然估计等\n",
    "    # 这里使用简单的平方损失作为示例\n",
    "    print(y_pred)\n",
    "    loss = np.sum((y - y_pred)**2)\n",
    "    return loss\n",
    "\n",
    "# 使用minimize进行拟合，传入自定义的损失函数\n",
    "initial_guess = [1.0, 1.0, 1.0]\n",
    "result = minimize(fun=custom_loss_function, x0=initial_guess, args=(x_data, y))\n",
    "\n",
    "# 输出拟合的参数\n",
    "print(result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.peak_prominences(data[:,1], [248, 269, 289, 305, 383],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_guess = []\n",
    "for i in peakind:\n",
    "    width=signal.peak_widths(data_arr, [i], rel_height=0.5)[0][0]\n",
    "    height=signal.peak_prominences(data_arr, [i])[0][0]\n",
    "    center = i\n",
    "    amplitude = height if height != 0 else 0.1\n",
    "    sigma = width/2.355 if width != 0 else 0.1\n",
    "    gamma = sigma/2 if width != 0 else 0.1\n",
    "    initial_guess.extend([center,amplitude,sigma,gamma])\n",
    "    print(center,amplitude,sigma,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_property,data[:,0][peakind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_half=signal.peak_widths(data[:,1], peakind, rel_height=0.5)\n",
    "plt.plot(data[:,1])\n",
    "plt.plot(peakind, data[:,1][peakind], \"x\")\n",
    "plt.hlines(*results_half[1:], color=\"C2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peakind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.peak_widths(data[:,1], [185], rel_height=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.peak_prominences(data[:,1],peakind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_full = signal.peak_widths(data[:,1], [185], rel_height=1)\n",
    "results_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:,0][peakind],len(peakind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.special import wofz\n",
    "\n",
    "# 定义Voigt函数\n",
    "def voigt(x, center, amplitude, sigma, gamma):\n",
    "    \"\"\"\n",
    "    Voigt函数是高斯函数和洛伦兹函数的卷积。\n",
    "    center: 峰的中心位置\n",
    "    amplitude: 峰的高度\n",
    "    sigma: 高斯分量的标准偏差\n",
    "    gamma: 洛伦兹分量的半宽度\n",
    "    \"\"\"\n",
    "    z = ((x-center) + 1j*gamma) / (sigma*np.sqrt(2))\n",
    "    return amplitude * np.real(wofz(z)) / (sigma*np.sqrt(2*np.pi))\n",
    "\n",
    "# 构建多个Voigt峰的组合函数\n",
    "def multiple_voigt(x, *params):\n",
    "    \"\"\"\n",
    "    params: 一个包含所有Voigt峰参数的列表，每个Voigt峰需要4个参数: center, amplitude, sigma, gamma\n",
    "    \"\"\"\n",
    "    y = np.zeros_like(x,dtype=np.float64)\n",
    "    for i in range(0, len(params), 4):\n",
    "        center = params[i]\n",
    "        amplitude = params[i+1]\n",
    "        sigma = params[i+2]\n",
    "        gamma = params[i+3]\n",
    "        y += voigt(x, center, amplitude, sigma, gamma)\n",
    "    return y\n",
    "\n",
    "# 假设的光谱数据及其噪声\n",
    "xdata = range(len(data_arr))\n",
    "ydata = data_arr\n",
    "\n",
    "# 初始猜测\n",
    "initial_guess = []\n",
    "for i in peakind:\n",
    "    width=signal.peak_widths(data_arr, [i], rel_height=0.5)[0][0]\n",
    "    height=signal.peak_prominences(data_arr, [i])[0][0]\n",
    "    amplitude = 20*data_arr[i]\n",
    "    sigma = width/2.355 if width != 0 else 10\n",
    "    gamma = sigma/16 if width != 0 else 1\n",
    "    initial_guess.extend([center,amplitude,sigma,gamma])\n",
    "    print(center,amplitude,sigma,gamma)\n",
    "\n",
    "# 执行拟合\n",
    "popt, pcov = curve_fit(multiple_voigt, xdata, ydata, p0=initial_guess, maxfev=10000,method='trf')\n",
    "\n",
    "# 输出最优拟合参数\n",
    "print(popt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_guess = []\n",
    "for i in peakind:\n",
    "    width=signal.peak_widths(data_arr, [i], rel_height=0.5)[0][0]\n",
    "    height=signal.peak_prominences(data_arr, [i])[0][0]\n",
    "    center = i\n",
    "    amplitude = 10*data_arr[i]*(1+height)\n",
    "    sigma = width if width != 0 else 10\n",
    "    gamma = sigma/100 if width != 0 else 0.02\n",
    "    initial_guess.extend([center,amplitude,sigma,gamma])\n",
    "    print(center,amplitude,sigma,gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xdata,ydata, \"c\")\n",
    "plt.plot(xdata,multiple_voigt(xdata,*initial_guess), \"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss=np.zeros_like([5,2])\n",
    "type(sss[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logger_config\n",
    "\n",
    "logger = logger_config.get_logger(__name__)\n",
    "logger.debug('This is a debug message')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c:\\\\Users\\\\chensq\\\\Desktop\\\\webApp\\\\webdist.1.0\\\\backend01',\n",
       " 'F:\\\\program\\\\chemoffice\\\\ChemScript\\\\Lib',\n",
       " 'f:\\\\program\\\\miniconda\\\\python311.zip',\n",
       " 'f:\\\\program\\\\miniconda\\\\DLLs',\n",
       " 'f:\\\\program\\\\miniconda\\\\Lib',\n",
       " 'f:\\\\program\\\\miniconda',\n",
       " '',\n",
       " 'f:\\\\program\\\\miniconda\\\\Lib\\\\site-packages',\n",
       " 'f:\\\\program\\\\miniconda\\\\Lib\\\\site-packages\\\\win32',\n",
       " 'f:\\\\program\\\\miniconda\\\\Lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'f:\\\\program\\\\miniconda\\\\Lib\\\\site-packages\\\\Pythonwin']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a04f5d07b0747026a8fbcdf50b9443318e69b1b8bd6247d88bfadb4789282972"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
